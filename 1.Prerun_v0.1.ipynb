{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Folder and pathway setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project folder '/home/pythagoras/Downloads/Test' already exists.\n",
      "Directory '/home/pythagoras/Downloads/Test/Input' created.\n",
      "Directory '/home/pythagoras/Downloads/Test/Output/Fasta' created.\n",
      "Directory '/home/pythagoras/Downloads/Test/Output/MSA/a3m' created.\n",
      "Directory '/home/pythagoras/Downloads/Test/Output/MSA/trimmed_a3m' created.\n",
      "Directory '/home/pythagoras/Downloads/Test/Output/MSA/sorted_a3m' created.\n",
      "Directory '/home/pythagoras/Downloads/Test/Output/MSA/combined_a3m' created.\n",
      "Directory '/home/pythagoras/Downloads/Test/Output/Clustering' created.\n",
      "Directory '/home/pythagoras/Downloads/Test/Output/AF2_Results' created.\n",
      "Directory '/home/pythagoras/Downloads/Test/Output/AF2_Results/zip_files' created.\n",
      "Directory '/home/pythagoras/Downloads/Test/Output/PSSM_Hits' created.\n",
      "\n",
      "Configuration:\n",
      "Base Directory: /home/pythagoras/Downloads/Test\n",
      "input: /home/pythagoras/Downloads/Test/Input\n",
      "output_fasta: /home/pythagoras/Downloads/Test/Output/Fasta\n",
      "output_a3m: /home/pythagoras/Downloads/Test/Output/MSA/a3m\n",
      "trunc_a3m: /home/pythagoras/Downloads/Test/Output/MSA/trimmed_a3m\n",
      "output_sorted_a3m: /home/pythagoras/Downloads/Test/Output/MSA/sorted_a3m\n",
      "output_combined_a3m: /home/pythagoras/Downloads/Test/Output/MSA/combined_a3m\n",
      "clustering: /home/pythagoras/Downloads/Test/Output/Clustering\n",
      "AF2_Results: /home/pythagoras/Downloads/Test/Output/AF2_Results\n",
      "AF2_zips: /home/pythagoras/Downloads/Test/Output/AF2_Results/zip_files\n",
      "output_search: /home/pythagoras/Downloads/Test/Output/PSSM_Hits\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "## Universal paths\n",
    "iupred_path = #\"/home/.../Programs/iupred3\"\n",
    "psipred_path = #\"/home/.../Programs/psipred\"\n",
    "NCBI_protein_database = #\"/home/.../Documents/PhD/PSSMSearch/protein.fasta\"\n",
    "bait_sequence = (\n",
    "    #\"INSERTSEQUENCEHERE\"\n",
    ")\n",
    "uniref90_path = #\"/home/.../Programs/Uniref/Uniref90/uniref90.fasta\"\n",
    "reformat_path = #\"/home/...]/Programs/hhsuite/hh-suite/build/scripts\"\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Prompts for project name and sets up the folder structure.\"\"\"\n",
    "    # Prompt for project name\n",
    "    project_name = input(\"Enter the project name: \").strip()\n",
    "    \n",
    "    # Define base directory\n",
    "    base_dir = os.path.join(os.getcwd(), project_name)\n",
    "    \n",
    "    # Create base directory if it doesn't exist\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "        print(f\"Project folder '{base_dir}' created.\")\n",
    "    else:\n",
    "        print(f\"Project folder '{base_dir}' already exists.\")\n",
    "    \n",
    "    # Define and create subdirectories\n",
    "    subdirs = {\n",
    "        \"input\": os.path.join(base_dir, \"Input\"),\n",
    "        \"output_fasta\": os.path.join(base_dir, \"Output/Fasta\"),\n",
    "        \"output_a3m\": os.path.join(base_dir, \"Output/MSA/a3m\"),\n",
    "        \"trunc_a3m\": os.path.join(base_dir, \"Output/MSA/trimmed_a3m\"),\n",
    "        \"output_sorted_a3m\": os.path.join(base_dir, \"Output/MSA/sorted_a3m\"),\n",
    "        \"output_combined_a3m\": os.path.join(base_dir, \"Output/MSA/combined_a3m\"),\n",
    "        \"clustering\": os.path.join(base_dir, \"Output/Clustering\"),\n",
    "        \"AF2_Results\": os.path.join(base_dir, \"Output/AF2_Results\"),\n",
    "        \"AF2_zips\": os.path.join(base_dir, \"Output/AF2_Results/zip_files\"),\n",
    "        \"output_search\": os.path.join(base_dir, \"Output/PSSM_Hits\")\n",
    "    }\n",
    "    \n",
    "    for name, path in subdirs.items():\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        print(f\"Directory '{path}' created.\")\n",
    "    \n",
    "    # Define path\n",
    "    input_sequences = os.path.join(base_dir, \"Input/input.fasta\")\n",
    "    pssm_output_path_blosum = os.path.join(base_dir, \"Output/pssm_BLOSUM62.csv\")\n",
    "    output_search_file = os.path.join(base_dir, \"Output/PSSM_Hits/Hits.fasta\")\n",
    "    output_search_file_nonred = os.path.join(base_dir, \"Output/PSSM_Hits/Hits_nonred.fasta\")\n",
    "    output_prey_bait = os.path.join(base_dir, \"Output/PSSM_Hits/PreyBait.fasta\")\n",
    "    output_fasta_split_directory = os.path.join(base_dir, \"Output/Fasta/\")\n",
    "    jackhmmer_output = os.path.join(base_dir, \"Output/MSA\")\n",
    "    a3m_output = os.path.join(base_dir, \"Output/MSA/a3m\")\n",
    "    sto_output = os.path.join(base_dir, \"Output/MSA/sto\")\n",
    "    truncated_a3m_output = os.path.join(base_dir, \"Output/MSA/trimmed_a3m\")\n",
    "    sorted_dir = os.path.join(base_dir, \"Output/MSA/sorted_a3m\")\n",
    "    combined_dir = os.path.join(base_dir, \"Output/MSA/combined_a3m\")\n",
    "\n",
    "    # Print the configuration\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(f\"Base Directory: {base_dir}\")\n",
    "    for key, value in subdirs.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    # Return the configuration\n",
    "    return base_dir, subdirs, input_sequences, pssm_output_path_blosum, output_search_file, output_prey_bait, output_fasta_split_directory, jackhmmer_output, a3m_output, sto_output, truncated_a3m_output, sorted_dir, combined_dir, output_search_file_nonred\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up the environment\n",
    "    base_dir, subdirectories, input_sequences, pssm_output_path_blosum, output_search_file, output_prey_bait, output_fasta_split_directory, jackhmmer_output, a3m_output, sto_output, truncated_a3m_output, sorted_dir, combined_dir, output_search_file_nonred = setup_environment()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. PSSM Generation with BLOSUM62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import blosum as bl\n",
    "\n",
    "# Define the input and output file paths\n",
    "filtered_sequences_path = input_sequences\n",
    "pssm_output_path = pssm_output_path_blosum\n",
    "\n",
    "# Load the BLOSUM62 matrix\n",
    "matrix = bl.BLOSUM(62)\n",
    "\n",
    "# Standard set of amino acids\n",
    "standard_amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "# Function to read sequences from a FASTA file\n",
    "def read_fasta_sequences(fasta_file):\n",
    "    sequences = [str(record.seq) for record in SeqIO.parse(fasta_file, \"fasta\")]\n",
    "    return sequences\n",
    "\n",
    "# Function to calculate the frequency matrix\n",
    "def calculate_frequency_matrix(sequences):\n",
    "    sequence_length = len(sequences[0])\n",
    "    amino_acids = sorted(set(standard_amino_acids))\n",
    "    \n",
    "    # Initialize the frequency matrix with zeros\n",
    "    frequency_matrix = np.zeros((len(amino_acids), sequence_length))\n",
    "    aa_to_index = {aa: idx for idx, aa in enumerate(amino_acids)}\n",
    "    \n",
    "    # Count the frequency of each amino acid at each position\n",
    "    for seq in sequences:\n",
    "        for i, aa in enumerate(seq):\n",
    "            if aa in aa_to_index:\n",
    "                frequency_matrix[aa_to_index[aa], i] += 1\n",
    "    \n",
    "    # Normalize the frequency matrix by the number of sequences\n",
    "    frequency_matrix /= len(sequences)\n",
    "    \n",
    "    return frequency_matrix, amino_acids\n",
    "\n",
    "# Function to generate the PSSM matrix from the frequency matrix using BLOSUM62 scores\n",
    "def generate_pssm_matrix(frequency_matrix, amino_acids, blosum62):\n",
    "    pssm_matrix = np.zeros(frequency_matrix.shape)\n",
    "    \n",
    "    for i in range(frequency_matrix.shape[1]):\n",
    "        for j, aa in enumerate(amino_acids):\n",
    "            score = 0\n",
    "            for k, aa2 in enumerate(amino_acids):\n",
    "                score += frequency_matrix[k, i] * blosum62[aa][aa2]\n",
    "            pssm_matrix[j, i] = score\n",
    "    \n",
    "    return pssm_matrix\n",
    "\n",
    "# Read the filtered sequences from the FASTA file\n",
    "sequences = read_fasta_sequences(filtered_sequences_path)\n",
    "\n",
    "# Calculate the frequency matrix\n",
    "frequency_matrix, amino_acids = calculate_frequency_matrix(sequences)\n",
    "\n",
    "# Generate the PSSM matrix\n",
    "pssm_matrix = generate_pssm_matrix(frequency_matrix, amino_acids, matrix)\n",
    "\n",
    "# Convert PSSM matrix to DataFrame for easier handling and export\n",
    "pssm_df = pd.DataFrame(pssm_matrix, index=list(amino_acids))\n",
    "\n",
    "# Ensure the DataFrame includes all standard amino acids\n",
    "for aa in standard_amino_acids:\n",
    "    if aa not in pssm_df.index:\n",
    "        pssm_df.loc[aa] = [0] * pssm_df.shape[1]\n",
    "\n",
    "# Reorder the DataFrame to match the standard amino acids order\n",
    "pssm_df = pssm_df.loc[list(standard_amino_acids)]\n",
    "\n",
    "# Output the PSSM matrix to a CSV file\n",
    "pssm_df.to_csv(pssm_output_path)\n",
    "\n",
    "print(f\"PSSM matrix saved to: {pssm_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Proteome Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" (A) Here, we will define the secondary structure optimization strategy and set the cutoff values.\"\"\"\n",
    "\n",
    "optimization = input(\"Please specify the optimization strategy for the secondary structure.\\nType 'helix', 'strand', 'coil' or 'unknown'.\").strip().lower()\n",
    "if optimization == \"helix\":\n",
    "    coil_cutoff = 0.9\n",
    "    helix_cutoff = 0.1\n",
    "    strand_cutoff = 0.9\n",
    "elif optimization == \"strand\":\n",
    "    coil_cutoff = 0.9\n",
    "    helix_cutoff = 0.9\n",
    "    strand_cutoff = 0.1\n",
    "elif optimization == \"coil\":\n",
    "    coil_cutoff = 0.1\n",
    "    helix_cutoff = 0.9\n",
    "    strand_cutoff = 0.9\n",
    "elif optimization == \"unknown\":\n",
    "    coil_cutoff = 0\n",
    "    helix_cutoff = 0\n",
    "    strand_cutoff = 0\n",
    "else:\n",
    "    raise ValueError(\"Invalid optimization strategy. Please specify 'helix', 'strand', 'coil' or 'unknown'.\")\n",
    "\n",
    "#Set the Cutoffs\n",
    "pssm_cutoff = 10\n",
    "iupred_cutoff = 0.40 \n",
    "anchor_cutoff = 0.40\n",
    "\n",
    "#Choose the Matrix you want to use, by removing the \"#\"\n",
    "pssm_matrix_path = pssm_output_path_blosum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" (B) Scores the Proteome.\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Define paths\n",
    "fasta_file = NCBI_protein_database\n",
    "output_fasta_file = output_search_file\n",
    "iupred_path = iupred_path\n",
    "psipred_path = psipred_path\n",
    "pssm_matrix_path = pssm_matrix_path\n",
    "\n",
    "# Function to read the PSSM matrix from a CSV file\n",
    "def read_pssm_matrix(pssm_matrix_path):\n",
    "    pssm_matrix = pd.read_csv(pssm_matrix_path, index_col=0)\n",
    "    return pssm_matrix\n",
    "\n",
    "# Function to read sequences from a FASTA file\n",
    "def read_fasta_sequences(fasta_file):\n",
    "    fasta_sequences = list(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "    return fasta_sequences\n",
    "\n",
    "# Function to cut sequence into fragments\n",
    "def cut_sequence_into_fragments(sequence, fragment_size):\n",
    "    sequence_length = len(sequence)\n",
    "    fragments = []\n",
    "    for i in range(sequence_length - fragment_size + 1):\n",
    "        fragment = sequence[i:i + fragment_size]\n",
    "        fragments.append((i, fragment))  # Include the start index\n",
    "    return fragments\n",
    "\n",
    "# Function to get IUPRED and ANCHOR scores\n",
    "def get_iupred_anchor_scores(sequence, seq_name, iupred_path):\n",
    "    temp_fasta = tempfile.NamedTemporaryFile(delete=False, mode='w', suffix='.fasta')\n",
    "    temp_fasta.write(f\">{seq_name}\\n{sequence}\\n\")\n",
    "    temp_fasta.close()\n",
    "    temp_fasta_name = temp_fasta.name\n",
    "\n",
    "    try:\n",
    "        # Run IUPred3 for disorder and ANCHOR scores\n",
    "        iupred_cmd = [\n",
    "            \"python3\",\n",
    "            os.path.join(iupred_path, \"iupred3.py\"),\n",
    "            temp_fasta_name,\n",
    "            \"long\",\n",
    "            \"--anchor\"\n",
    "        ]\n",
    "        result = subprocess.run(iupred_cmd, capture_output=True, text=True)\n",
    "\n",
    "        if result.returncode != 0:\n",
    "            print(f\"Error running IUPred3 for {seq_name}: {result.stderr}\")\n",
    "            return None, None\n",
    "\n",
    "        iupred_scores, anchor_scores = [], []\n",
    "        for line in result.stdout.splitlines():\n",
    "            if line.startswith('#') or line.strip() == '':\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) == 4:\n",
    "                iupred_scores.append(float(parts[2]))  # IUPRED score\n",
    "                anchor_scores.append(float(parts[3]))  # ANCHOR score\n",
    "        return iupred_scores, anchor_scores\n",
    "    finally:\n",
    "        os.unlink(temp_fasta_name)\n",
    "\n",
    "# Function to get secondary structure probabilities from PSIPRED\n",
    "def get_psipred_probabilities(seq_id, sequence, psipred_path):\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        original_dir = os.getcwd()\n",
    "        try:\n",
    "            os.chdir(temp_dir)\n",
    "            temp_fasta_name = 'input.fasta'\n",
    "            with open(temp_fasta_name, 'w') as temp_fasta:\n",
    "                temp_fasta.write(f\">{seq_id}\\n{sequence}\\n\")\n",
    "            \n",
    "            env = os.environ.copy()\n",
    "            env['PSIPRED_DATA'] = os.path.join(psipred_path, 'data')\n",
    "            env['PATH'] += ':/usr/bin'  # Ensure BLAST is available in PATH\n",
    "            \n",
    "            runpsipred_cmd = [\n",
    "                os.path.join(psipred_path, \"runpsipred_single\"),\n",
    "                temp_fasta_name\n",
    "            ]\n",
    "            result = subprocess.run(runpsipred_cmd, capture_output=True, text=True, env=env)\n",
    "            \n",
    "            if result.returncode != 0:\n",
    "                print(f\"Error running PSIPRED for {seq_id}: {result.stderr}\")\n",
    "                return None\n",
    "\n",
    "            ss2_file = 'input.ss2'\n",
    "            if not os.path.exists(ss2_file):\n",
    "                return None\n",
    "\n",
    "            probabilities = []\n",
    "            with open(ss2_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    if line.startswith('#') or line.strip() == '':\n",
    "                        continue\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) != 6:\n",
    "                        continue\n",
    "                    prob_c = float(parts[3])  # Coil\n",
    "                    prob_h = float(parts[4])  # Helix\n",
    "                    prob_e = float(parts[5])  # Strand\n",
    "                    probabilities.append({'Prob_C': prob_c, 'Prob_H': prob_h, 'Prob_E': prob_e})\n",
    "            return probabilities\n",
    "        finally:\n",
    "            os.chdir(original_dir)\n",
    "\n",
    "def pssm_score_sequence(sequence, pssm_matrix):\n",
    "    valid_amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")  # The standard 20 amino acids\n",
    "    score = 0\n",
    "    for i in range(len(sequence)):\n",
    "        aa = sequence[i]\n",
    "        column_name = str(i)\n",
    "        if aa not in valid_amino_acids:\n",
    "            print(f\"Skipping fragment due to invalid amino acid '{aa}' in sequence '{sequence}'\")\n",
    "            return None\n",
    "        if aa in pssm_matrix.index and column_name in pssm_matrix.columns:\n",
    "            score += pssm_matrix.at[aa, column_name]\n",
    "        else:\n",
    "            print(f\"Invalid amino acid '{aa}' or column '{column_name}' in sequence '{sequence}'\")\n",
    "            return None\n",
    "    return score\n",
    "\n",
    "# Function to process a single sequence record, modified to collect all hits\n",
    "def process_sequence_record(args):\n",
    "    (seq_record, pssm_matrix, iupred_path, psipred_path, pssm_cutoff, \n",
    "     iupred_cutoff, anchor_cutoff, ss_cutoff, flanking_aa_size, flank_size, optimization) = args\n",
    "    sequence_name = seq_record.id\n",
    "    full_header = seq_record.description\n",
    "    sequence = str(seq_record.seq)\n",
    "\n",
    "    # Initialize variables to store IUPRED and ANCHOR scores, and PSIPRED probabilities\n",
    "    iupred_scores = None\n",
    "    anchor_scores = None\n",
    "    psipred_probs = None\n",
    "\n",
    "    fragment_size = pssm_matrix.shape[1]\n",
    "    fragments = cut_sequence_into_fragments(sequence, fragment_size)\n",
    "\n",
    "    hits = []  # List to store all hits for the current sequence\n",
    "\n",
    "    # Loop over fragments\n",
    "    for i, fragment in fragments:\n",
    "        pssm_score = pssm_score_sequence(fragment, pssm_matrix)\n",
    "\n",
    "        # Check if the pssm_score is valid before proceeding\n",
    "        if pssm_score is None:\n",
    "            continue  # Skip this fragment if pssm_score is None\n",
    "\n",
    "        if pssm_score >= pssm_cutoff:\n",
    "            # If IUPRED and ANCHOR scores are not computed yet, compute them for the whole sequence\n",
    "            if iupred_scores is None or anchor_scores is None:\n",
    "                iupred_scores, anchor_scores = get_iupred_anchor_scores(sequence, sequence_name, iupred_path)\n",
    "                if iupred_scores is None or anchor_scores is None:\n",
    "                    print(f\"Skipping sequence {sequence_name} due to IUPRED/ANCHOR failure.\")\n",
    "                    return None  # Skip this sequence if IUPRED/ANCHOR failed\n",
    "\n",
    "            # Define flanking regions\n",
    "            start_flank = max(0, i - flank_size)\n",
    "            end_flank = min(len(sequence), i + fragment_size + flank_size)\n",
    "\n",
    "            # Exclude the fragment itself from the IUPRED score calculation\n",
    "            left_flank_scores = iupred_scores[start_flank:i]  # Flanking region before the fragment\n",
    "            right_flank_scores = iupred_scores[i + fragment_size:end_flank]  # Flanking region after the fragment\n",
    "            flanking_region_scores = left_flank_scores + right_flank_scores\n",
    "\n",
    "            if len(flanking_region_scores) == 0:\n",
    "                mean_iupred = 0  # Adjust as needed\n",
    "            else:\n",
    "                mean_iupred = np.mean(flanking_region_scores)\n",
    "\n",
    "            # Calculate mean ANCHOR score over the fragment itself\n",
    "            fragment_anchor_scores = anchor_scores[i:i+fragment_size]\n",
    "            mean_anchor = np.mean(fragment_anchor_scores)\n",
    "\n",
    "            if mean_iupred >= iupred_cutoff and mean_anchor >= anchor_cutoff:\n",
    "                # If PSIPRED probabilities are not computed yet, compute them for the whole sequence\n",
    "                if psipred_probs is None:\n",
    "                    psipred_probs = get_psipred_probabilities(sequence_name, sequence, psipred_path)\n",
    "                    if psipred_probs is None:\n",
    "                        print(f\"Skipping sequence {sequence_name} due to PSIPRED failure.\")\n",
    "                        return None  # Skip this sequence if PSIPRED failed\n",
    "\n",
    "                # Extract probabilities for the fragment positions\n",
    "                fragment_psipred_probs = psipred_probs[i:i+fragment_size]\n",
    "\n",
    "                mean_coil = np.mean([p['Prob_C'] for p in fragment_psipred_probs])\n",
    "                mean_helix = np.mean([p['Prob_H'] for p in fragment_psipred_probs])\n",
    "                mean_strand = np.mean([p['Prob_E'] for p in fragment_psipred_probs])\n",
    "\n",
    "                fragment_passes_filters = False  # Initialize to False\n",
    "                if optimization == \"helix\":\n",
    "                    if (mean_coil < ss_cutoff['coil'] and mean_helix >= ss_cutoff['helix'] and mean_strand < ss_cutoff['strand']):\n",
    "                        fragment_passes_filters = True\n",
    "\n",
    "                elif optimization == \"strand\":\n",
    "                    if (mean_coil < ss_cutoff['coil'] and mean_helix < ss_cutoff['helix'] and mean_strand >= ss_cutoff['strand']):\n",
    "                        fragment_passes_filters = True\n",
    "\n",
    "\n",
    "                elif optimization == \"coil\":\n",
    "                    if (mean_coil >= ss_cutoff['coil'] and mean_helix < ss_cutoff['helix'] and mean_strand < ss_cutoff['strand']):\n",
    "                        fragment_passes_filters = True\n",
    " \n",
    "                elif optimization == \"unknown\":\n",
    "                    if (mean_coil > ss_cutoff['coil'] and mean_helix > ss_cutoff['helix'] and mean_strand > ss_cutoff['strand']):    \n",
    "                        fragment_passes_filters = True # \n",
    "\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid optimization strategy. Please specify 'helix', 'strand', 'coil' or 'unknown'.\")\n",
    "                \n",
    "                if fragment_passes_filters:\n",
    "                    # Include flanking amino acids from the original sequence\n",
    "                    start_output_flank = max(0, i - flanking_aa_size)\n",
    "                    end_output_flank = min(len(sequence), i + fragment_size + flanking_aa_size)\n",
    "                    fragment_with_output_flanks = sequence[start_output_flank:end_output_flank]\n",
    "\n",
    "                    fasta_header = (f\">{full_header} pos:{i+1} PSSM_Score:{pssm_score:.2f} \"\n",
    "                                    f\"IUPRED_Score:{mean_iupred:.2f} ANCHOR_Score:{mean_anchor:.2f} \"\n",
    "                                    f\"Coil:{mean_coil:.2f} Helix:{mean_helix:.2f} Strand:{mean_strand:.2f}\")\n",
    "\n",
    "                    # Append this fragment to the hits list\n",
    "                    hits.append((fasta_header, fragment_with_output_flanks))\n",
    "\n",
    "    return hits  # Return the list of hits for this sequence\n",
    "\n",
    "def main(pssm_cutoff, iupred_cutoff, anchor_cutoff, coil_cutoff, helix_cutoff, strand_cutoff):\n",
    "    # Load PSSM matrix\n",
    "    pssm_matrix = read_pssm_matrix(pssm_matrix_path)\n",
    "    print(f\"PSSM Matrix Columns: {pssm_matrix.columns.tolist()}\")\n",
    "\n",
    "    # Read the sequences from the FASTA file\n",
    "    sequences_to_score = read_fasta_sequences(fasta_file)\n",
    "\n",
    "    # Define the cutoffs and parameters\n",
    "    pssm_cutoff, iupred_cutoff, anchor_cutoff = pssm_cutoff, iupred_cutoff, anchor_cutoff \n",
    "    ss_cutoff = {'coil': coil_cutoff, 'helix': helix_cutoff, 'strand': strand_cutoff}\n",
    "    flanking_aa_size = 20  # Number of flanking amino acids to include when writing output\n",
    "    flank_size = 60       # Flank size for IUPRED calculation (same as in Code B)\n",
    "\n",
    "    # Create args list\n",
    "    args = [\n",
    "        (\n",
    "            seq_record, pssm_matrix, iupred_path, psipred_path, \n",
    "            pssm_cutoff, iupred_cutoff, anchor_cutoff, ss_cutoff, \n",
    "            flanking_aa_size, flank_size, optimization\n",
    "        ) \n",
    "        for seq_record in sequences_to_score\n",
    "    ]\n",
    "\n",
    "    # Perform parallel processing\n",
    "    num_cores = cpu_count() - 1\n",
    "    print(f\"Using {num_cores} cores for processing.\")\n",
    "    with Pool(num_cores) as pool:\n",
    "        all_results = pool.map(process_sequence_record, args)\n",
    "\n",
    "    # Write all hits from each sequence to the output file\n",
    "    with open(output_fasta_file, \"w\") as output_conn:\n",
    "        for result in all_results:\n",
    "            if result:  # Only write if there were hits found\n",
    "                for header, fragment in result:\n",
    "                    output_conn.write(f\"{header}\\n{fragment}\\n\")\n",
    "\n",
    "    print(f\"All hits written to {output_fasta_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(pssm_cutoff, iupred_cutoff, anchor_cutoff, coil_cutoff, helix_cutoff, strand_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" (C) Remove redundant sequences in the PSSM-hits file. This avoids running identical sequences through jackhmmer and ColabFold multiple times.\"\"\"\n",
    "\n",
    "\n",
    "def read_fasta_sequences(file_path):\n",
    "    \"\"\"\n",
    "    Reads sequences from a FASTA file and returns a dictionary with sequences as keys and headers as values.\n",
    "    If a sequence appears multiple times with different headers, only the first header is kept.\n",
    "    \"\"\"\n",
    "    sequences = {}\n",
    "    with open(file_path, \"r\") as file:\n",
    "        header = None\n",
    "        sequence_data = []\n",
    "        for line in file:\n",
    "            if line.startswith(\">\"):\n",
    "                # If there is an existing sequence, add it to the dictionary\n",
    "                if header and sequence_data:\n",
    "                    sequence = \"\".join(sequence_data)\n",
    "                    # Add the sequence to the dictionary if it's not already present\n",
    "                    if sequence not in sequences:\n",
    "                        sequences[sequence] = header\n",
    "                # Start a new sequence\n",
    "                header = line.strip()  # Keep the full header, including '>'\n",
    "                sequence_data = []\n",
    "            else:\n",
    "                sequence_data.append(line.strip())\n",
    "        # Add the last sequence to the dictionary\n",
    "        if header and sequence_data:\n",
    "            sequence = \"\".join(sequence_data)\n",
    "            if sequence not in sequences:\n",
    "                sequences[sequence] = header\n",
    "    return sequences\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the input and output file paths\n",
    "    input_file_path = output_search_file  \n",
    "    output_file_path = output_search_file_nonred\n",
    "\n",
    "    # Read the sequences from the input file\n",
    "    sequences_dict = read_fasta_sequences(input_file_path)\n",
    "\n",
    "    # Write the unique sequences with their original headers to the output file\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "        for sequence, header in sequences_dict.items():\n",
    "            output_file.write(f\"{header}\\n{sequence}\\n\")\n",
    "\n",
    "    print(f\"{len(sequences_dict)} non-redundant sequences written to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" (Optional) This will compare the PSSM-hits of two iterations and write the unique hits to a new FASTA file. Please ignore this cell in case you are running the first iteration.\"\"\"\n",
    "\n",
    "\n",
    "# File paths for your two FASTA files\n",
    "file_iteration01 = # Path to your first iteration PSSM file\n",
    "file_iteration02 = # Path to your second iteration PSSM file\n",
    "output_file = os.path.join(base_dir, \"Output/PSSM/Unique_PSSM.fasta\")  # Output file for unique sequences\n",
    "\n",
    "def read_fasta_sequences(file_path):\n",
    "    \"\"\"\n",
    "    Reads sequences from a FASTA file and returns a dictionary with sequence IDs as keys and sequences as values.\n",
    "    \"\"\"\n",
    "    sequences = {}\n",
    "    with open(file_path, \"r\") as file:\n",
    "        sequence_id = None\n",
    "        sequence_data = []\n",
    "        for line in file:\n",
    "            if line.startswith(\">\"):\n",
    "                if sequence_id and sequence_data:\n",
    "                    sequences[sequence_id] = \"\".join(sequence_data)\n",
    "                sequence_id = line[1:].strip()\n",
    "                sequence_data = []\n",
    "            else:\n",
    "                sequence_data.append(line.strip())\n",
    "        if sequence_id and sequence_data:\n",
    "            sequences[sequence_id] = \"\".join(sequence_data)\n",
    "    return sequences\n",
    "\n",
    "\n",
    "# Read sequences from both files\n",
    "sequences_iteration01 = read_fasta_sequences(file_iteration01)\n",
    "sequences_iteration02 = read_fasta_sequences(file_iteration02)\n",
    "\n",
    "# Create a set of sequences in Iteration 01 to check against\n",
    "sequences_set_iteration01 = set(sequences_iteration01.values())\n",
    "\n",
    "# Find new hits in Iteration 02 that have unique sequences (not in Iteration 01)\n",
    "unique_sequences_iteration02 = {\n",
    "    seq_id: seq\n",
    "    for seq_id, seq in sequences_iteration02.items()\n",
    "    if seq not in sequences_set_iteration01\n",
    "}\n",
    "\n",
    "# Write unique sequences to a new FASTA file\n",
    "with open(output_file, \"w\") as output:\n",
    "    for seq_id, sequence in unique_sequences_iteration02.items():\n",
    "        output.write(f\">{seq_id}\\n{sequence}\\n\")\n",
    "\n",
    "output_search_file_nonred = output_file  # Update the variable to point to the new file\n",
    "print(f\"Unique sequences have been written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Bait Fusion and Prey-Bait Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_fasta_path = output_search_file_nonred\n",
    "\n",
    "def process_fasta(input_path, output_path, bait_sequence):\n",
    "    # Read and format the sequences\n",
    "    formatted_sequences = []\n",
    "    \n",
    "    with open(input_path, \"r\") as file:\n",
    "        protein_name = None\n",
    "        protein_sequence = []\n",
    "        \n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">\"):\n",
    "                if protein_name:  # Save the previous protein sequence if exists\n",
    "                    combined_sequence = \"\".join(protein_sequence) + \":\" + bait_sequence\n",
    "                    formatted_sequences.append((protein_name, combined_sequence))\n",
    "                    \n",
    "                protein_name = line  # Start a new protein sequence\n",
    "                protein_sequence = []\n",
    "            else:\n",
    "                protein_sequence.append(line)\n",
    "    \n",
    "        # Handle the last protein sequence\n",
    "        if protein_name:\n",
    "            combined_sequence = \"\".join(protein_sequence) + \":\" + bait_sequence\n",
    "            formatted_sequences.append((protein_name, combined_sequence))\n",
    "\n",
    "\n",
    "    # Write the formatted (and possibly curated) sequences to the output file\n",
    "    with open(output_path, \"w\") as outfile:\n",
    "        for name, seq in formatted_sequences:\n",
    "            outfile.write(name + \"\\n\" + seq + \"\\n\")\n",
    "\n",
    "    return formatted_sequences\n",
    "\n",
    "# Usage example\n",
    "bait_sequence = bait_sequence\n",
    "output_fasta_path = output_prey_bait\n",
    "\n",
    "processed_sequences = process_fasta(input_fasta_path, output_fasta_path, bait_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Split Prey-Bait pairs into individual FASTA files for ColabFold input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This will split the PreyBait.fasta file into individual FASTA files to generate the input for ColabFold.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"Sanitizes the filename by replacing any characters that are not allowed in filenames.\"\"\"\n",
    "    s = re.sub(r'[^0-9a-zA-Z_\\.]+', '_', filename)\n",
    "    return s.strip('_')\n",
    "\n",
    "\n",
    "def extract_accession_and_position(identifier):\n",
    "    \"\"\"\n",
    "    Extracts the accession number and position from the FASTA header.\n",
    "    \"\"\"\n",
    "    # Remove the leading '>' if present\n",
    "    identifier = identifier.lstrip('>')\n",
    "\n",
    "    # Use regex to find the accession number and position\n",
    "    match = re.search(r'^(\\S+).*?pos:(\\d+)', identifier)\n",
    "    if match:\n",
    "        accession = match.group(1)  # Extract accession number (e.g., NP_001375333.1)\n",
    "        position = match.group(2)   # Extract position number (e.g., 143)\n",
    "        filename = f\"{accession}_pos_{position}\"\n",
    "        return sanitize_filename(filename)\n",
    "    else:\n",
    "        # Fallback: sanitize and shorten the entire identifier\n",
    "        return sanitize_filename(identifier)[:50]  # Limit to 50 characters to avoid long filenames\n",
    "\n",
    "\n",
    "def split_fasta_to_files(input_file, output_directory):\n",
    "    \"\"\"\n",
    "    Splits a combined FASTA file into individual files for each sequence, naming the files based on accession number and position.\n",
    "\n",
    "    Parameters:\n",
    "    - input_file: Path to the combined FASTA file.\n",
    "    - output_directory: Directory where individual FASTA files will be saved.\n",
    "    - max_entries (optional): Maximum number of FASTA entries to process. Defaults to processing all entries.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Read the combined FASTA file\n",
    "    with open(input_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Process the lines to split into identifiers and sequences\n",
    "    identifiers = [lines[i].strip() for i in range(0, len(lines), 2)]\n",
    "    sequences = [lines[i].strip() for i in range(1, len(lines), 2)]\n",
    "\n",
    "    # Extract filenames based on accession number and position\n",
    "    filenames = [extract_accession_and_position(identifier) for identifier in identifiers]\n",
    "\n",
    "    # Write individual FASTA files\n",
    "    for filename, identifier, sequence in zip(filenames, identifiers, sequences):\n",
    "        filepath = os.path.join(output_directory, f\"{filename}.fasta\")\n",
    "\n",
    "        with open(filepath, 'w') as file:\n",
    "            file.write(f\"{identifier}\\n{sequence}\\n\")\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "input_path = output_prey_bait\n",
    "output_directory = output_fasta_split_directory\n",
    "\n",
    "split_fasta_to_files(input_path, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Multiple Sequence Alignment for Bait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"Sanitizes the filename by replacing any characters that are not allowed in filenames.\"\"\"\n",
    "    s = re.sub(r'[^0-9a-zA-Z_\\.]+', '_', filename)\n",
    "    return s.strip('_')\n",
    "\n",
    "def safe_makedirs(directory):\n",
    "    \"\"\"Creates a directory safely, avoiding race conditions.\"\"\"\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure the necessary variables are defined\n",
    "    try:\n",
    "        bait_sequence\n",
    "        uniref90_path\n",
    "        jackhmmer_output\n",
    "    except NameError as e:\n",
    "        logging.error(f\"Required variable not defined: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Prepare the bait sequence\n",
    "    bait_sequence_header = \">bait_sequence\"\n",
    "    sequence = bait_sequence.strip()\n",
    "\n",
    "    # Sanitize header to create filename\n",
    "    filename = sanitize_filename(bait_sequence_header.lstrip('>'))\n",
    "\n",
    "    # Define output directories\n",
    "    output_dir = jackhmmer_output  # Should be defined from setup_environment()\n",
    "    sto_dir = os.path.join(output_dir, \"sto\")\n",
    "    tbl_dir = os.path.join(output_dir, \"tbl\")\n",
    "    safe_makedirs(sto_dir)\n",
    "    safe_makedirs(tbl_dir)\n",
    "\n",
    "    sto_output_file = os.path.join(sto_dir, f\"{filename}.sto\")\n",
    "    tbl_output_file = os.path.join(tbl_dir, f\"{filename}_output.tbl\")\n",
    "\n",
    "    fasta_file = f\"{filename}.fasta\"\n",
    "    with open(fasta_file, 'w') as f:\n",
    "        f.write(f\"{bait_sequence_header}\\n{sequence}\\n\")\n",
    "\n",
    "    # Run jackhmmer\n",
    "    # Number of CPUs to use\n",
    "    num_cpus = cpu_count() - 1  # Adjust as needed\n",
    "\n",
    "    # Use the -E option to restrict based on E-value\n",
    "    cmd = (\n",
    "    f\"jackhmmer --cpu {num_cpus} -E 0.0001 -N 1 \"\n",
    "    f\"--F1 0.0005 --F2 0.00005 --F3 0.0000005 --noali \"\n",
    "    f\"--tblout {tbl_output_file} -A {sto_output_file} {fasta_file} {uniref90_path}\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"Running jackhmmer for bait sequence\")\n",
    "        subprocess.run(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n",
    "        logging.info(f\"Completed jackhmmer for bait sequence\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logging.error(f\"Jackhmmer failed for bait sequence: {e}\")\n",
    "    finally:\n",
    "        os.remove(fasta_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Multiple Sequence Alignment for Peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "from multiprocessing import Pool, cpu_count, Manager, Lock\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"Sanitizes the filename by replacing any characters that are not allowed in filenames.\"\"\"\n",
    "    s = re.sub(r'[^0-9a-zA-Z_\\.]+', '_', filename)\n",
    "    return s.strip('_')\n",
    "\n",
    "def extract_accession_and_position(identifier):\n",
    "    \"\"\"\n",
    "    Extracts the accession number and position from the FASTA header.\n",
    "    \"\"\"\n",
    "    identifier = identifier.lstrip('>')\n",
    "    match = re.search(r'^(\\S+).*?pos:(\\d+)', identifier)\n",
    "    if match:\n",
    "        accession = match.group(1)\n",
    "        position = match.group(2)\n",
    "        filename = f\"{accession}_pos_{position}\"\n",
    "        return sanitize_filename(filename)\n",
    "    else:\n",
    "        return sanitize_filename(identifier)[:50]\n",
    "\n",
    "def safe_makedirs(directory):\n",
    "    \"\"\"Creates a directory safely, avoiding race conditions.\"\"\"\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def write_remaining_fasta(remaining_dict, remaining_fasta_path):\n",
    "    \"\"\"\n",
    "    Writes the current sequences in remaining_dict to the input_remaining.fasta file.\n",
    "    remaining_dict is a dictionary: {header: sequence}\n",
    "    \"\"\"\n",
    "    with open(remaining_fasta_path, 'w') as f:\n",
    "        for header, seq in remaining_dict.items():\n",
    "            f.write(f\"{header}\\n{seq}\\n\")\n",
    "\n",
    "def run_jackhmmer(args):\n",
    "    \"\"\"\n",
    "    Runs jackhmmer on the given sequence and writes the output to a file.\n",
    "    Updates the input_remaining.fasta upon successful completion.\n",
    "    \"\"\"\n",
    "    (sequence, header, output_dir, seqdb, num_cpus, lock, remaining_fasta_path, remaining_dict) = args\n",
    "\n",
    "    sto_dir = os.path.join(output_dir, \"sto\")\n",
    "    tbl_dir = os.path.join(output_dir, \"tbl\")\n",
    "    safe_makedirs(sto_dir)\n",
    "    safe_makedirs(tbl_dir)\n",
    "\n",
    "    filename = extract_accession_and_position(header)\n",
    "    sto_output_file = os.path.join(sto_dir, f\"{filename}.sto\")\n",
    "    tbl_output_file = os.path.join(tbl_dir, f\"{filename}_output.tbl\")\n",
    "\n",
    "    # Temporary FASTA file for this sequence\n",
    "    fasta_file = f\"{filename}.fasta\"\n",
    "    with open(fasta_file, 'w') as f:\n",
    "        f.write(f\"{header}\\n{sequence}\\n\")\n",
    "\n",
    "    cmd = (\n",
    "    f\"jackhmmer --cpu {num_cpus} -E 0.0001 -N 1 \"\n",
    "    f\"--F1 0.0005 --F2 0.00005 --F3 0.0000005 --noali \"\n",
    "    f\"-A {sto_output_file} --tblout {tbl_output_file} {fasta_file} {seqdb}\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"Running jackhmmer for {header}\")\n",
    "        subprocess.run(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n",
    "        logging.info(f\"Completed jackhmmer for {header}\")\n",
    "\n",
    "        # On success, remove this sequence from the remaining_dict and update the file\n",
    "        with lock:\n",
    "            if header in remaining_dict:\n",
    "                del remaining_dict[header]\n",
    "                write_remaining_fasta(remaining_dict, remaining_fasta_path)\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logging.error(f\"Jackhmmer failed for {header}: {e}\")\n",
    "    finally:\n",
    "        os.remove(fasta_file)\n",
    "\n",
    "    return sto_output_file, tbl_output_file\n",
    "\n",
    "def parse_fasta(fasta_file):\n",
    "    \"\"\"\n",
    "    Parses the FASTA file and returns a list of (sequence, header) tuples.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    with open(fasta_file, 'r') as f:\n",
    "        header = None\n",
    "        sequence = \"\"\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if header and sequence:\n",
    "                    sequences.append((sequence, header))\n",
    "                header = line\n",
    "                sequence = \"\"\n",
    "            else:\n",
    "                sequence += line\n",
    "        # Add the last sequence\n",
    "        if header and sequence:\n",
    "            sequences.append((sequence, header))\n",
    "    return sequences\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the input FASTA file, output directory, and the sequence database\n",
    "    fasta_file = output_search_file_nonred\n",
    "    output_dir = jackhmmer_output\n",
    "    seqdb = uniref90_path  # Path to the database you want to search against\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    safe_makedirs(output_dir)\n",
    "\n",
    "    # Parse the FASTA file to get the sequences\n",
    "    sequences = parse_fasta(fasta_file)\n",
    "\n",
    "    # Initialize the remaining sequences dictionary {header: sequence}\n",
    "    # This will help us keep track of what still needs to be run.\n",
    "    manager = Manager()\n",
    "    remaining_dict = manager.dict({header: seq for (seq, header) in sequences})\n",
    "\n",
    "    # Write initial remaining.fasta file containing all sequences\n",
    "    remaining_fasta_path = os.path.join(os.path.dirname(fasta_file), \"input_remaining.fasta\")\n",
    "    with open(remaining_fasta_path, 'w') as f:\n",
    "        for seq, header in sequences:\n",
    "            f.write(f\"{header}\\n{seq}\\n\")\n",
    "\n",
    "    # Number of CPUs to use\n",
    "    total_cpus = cpu_count()\n",
    "    # Number of CPUs per jackhmmer process\n",
    "    num_cpus_per_process = 1         # Adjust as needed\n",
    "    # Number of jackhmmer processes to run in parallel\n",
    "    num_processes = (total_cpus // num_cpus_per_process) - 1\n",
    "\n",
    "    lock = manager.Lock()\n",
    "\n",
    "    # Prepare arguments for run_jackhmmer\n",
    "    args_list = [(seq, header, output_dir, seqdb, num_cpus_per_process, lock, remaining_fasta_path, remaining_dict) \n",
    "                 for seq, header in sequences]\n",
    "\n",
    "    # Preload the database using vmtouch\n",
    "    # This command will \"touch\" all pages of seqdb to bring them into RAM if possible.\n",
    "    #vmtouch_cmd = [\"vmtouch\", \"-t\", seqdb]\n",
    "    #logging.info(\"Preloading database into RAM using vmtouch...\")\n",
    "    #subprocess.run(vmtouch_cmd, check=True)\n",
    "    #logging.info(\"Finished preloading database into RAM.\")\n",
    "\n",
    "    # Run jackhmmer in parallel\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        pool.map(run_jackhmmer, args_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Convert the .sto to .a3m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "\n",
    "def convert_sto_to_a3m(sto_file):\n",
    "    \"\"\"\n",
    "    Converts a .sto file to an .a3m file using reformat.pl.\n",
    "    Parameters:\n",
    "    - sto_file: Path to the input .sto file.\n",
    "    \"\"\"\n",
    "    # Define the script path and output directory inside the function\n",
    "    script_path = reformat_path\n",
    "    output_dir = a3m_output\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    # Extract the filename without the extension\n",
    "    filename = os.path.basename(sto_file).replace(\".sto\", \"\")\n",
    "    # Define the output .a3m file path\n",
    "    a3m_file = os.path.join(output_dir, f\"{filename}.a3m\")\n",
    "    # Build the reformat.pl command\n",
    "    cmd = f\"perl {script_path}/reformat.pl sto a3m {sto_file} {a3m_file}\"\n",
    "    # Run the command\n",
    "    subprocess.run(cmd, shell=True)\n",
    "    print(f\"Converted {sto_file} to {a3m_file}\")\n",
    "    return a3m_file\n",
    "\n",
    "def process_sto_files(sto_dir, num_processes):\n",
    "    \"\"\"\n",
    "    Processes all .sto files in a directory and converts them to .a3m files in parallel.\n",
    "    Parameters:\n",
    "    - sto_dir: Directory containing the .sto files.\n",
    "    - num_processes: Number of worker processes to use.\n",
    "    \"\"\"\n",
    "    # Get a list of all .sto files\n",
    "    sto_files = [os.path.join(sto_dir, filename) for filename in os.listdir(sto_dir) if filename.endswith(\".sto\")]\n",
    "    # Use multiprocessing Pool to process files in parallel\n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        pool.map(convert_sto_to_a3m, sto_files)\n",
    "\n",
    "# Define the directory containing .sto files\n",
    "sto_dir = sto_output\n",
    "# Specify the number of cores you want to use\n",
    "num_processes = 12  # Change this to the desired number of cores\n",
    "# Process all .sto files in the directory\n",
    "process_sto_files(sto_dir, num_processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Sort and Deduplicate a3m files Based on Sequence Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# Define paths\n",
    "input_dir = a3m_output\n",
    "sorted_dir = sorted_dir\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(sorted_dir, exist_ok=True)\n",
    "\n",
    "def calculate_sequence_identity(seq1, seq2):\n",
    "    \"\"\"Calculates the global sequence identity between two sequences, comparing up to the shortest sequence length.\"\"\"\n",
    "    min_length = min(len(seq1), len(seq2))\n",
    "    seq1_upper = seq1[:min_length].upper()\n",
    "    seq2_upper = seq2[:min_length].upper()\n",
    "    matches = sum(res1 == res2 for res1, res2 in zip(seq1_upper, seq2_upper))\n",
    "    identity = matches / min_length\n",
    "    return identity\n",
    "\n",
    "def parse_a3m_to_dataframe(file_path):\n",
    "    \"\"\"Parses an .a3m file into a DataFrame of headers and sequences.\"\"\"\n",
    "    headers = []\n",
    "    sequences = []\n",
    "    reference_length = None\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        header = ''\n",
    "        seq_lines = []\n",
    "        \n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if header and seq_lines:\n",
    "                    seq = ''.join(seq_lines).replace(\" \", \"\").replace(\"\\t\", \"\")\n",
    "                    headers.append(header)\n",
    "                    sequences.append(seq)\n",
    "                    \n",
    "                    if reference_length is None:\n",
    "                        reference_length = len(seq)\n",
    "                        \n",
    "                header = line[1:].split()[0]\n",
    "                seq_lines = []\n",
    "            else:\n",
    "                seq_lines.append(line)\n",
    "        \n",
    "        # Add the last sequence\n",
    "        if header and seq_lines:\n",
    "            seq = ''.join(seq_lines).replace(\" \", \"\").replace(\"\\t\", \"\")\n",
    "            headers.append(header)\n",
    "            sequences.append(seq)\n",
    "            if reference_length is None:\n",
    "                reference_length = len(seq)\n",
    "    \n",
    "    df = pd.DataFrame({'header': headers, 'sequence': sequences})\n",
    "    return df\n",
    "\n",
    "def sort_a3m_file(args):\n",
    "    \"\"\"\n",
    "    Reads an .a3m file, optionally deduplicates sequences, optionally sorts by\n",
    "    global sequence identity to the first (reference) sequence, and saves the result.\n",
    "    \"\"\"\n",
    "    file_path, output_dir, deduplicate, do_sort = args\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "\n",
    "    # Parse the a3m\n",
    "    df = parse_a3m_to_dataframe(file_path)\n",
    "    if df.empty:\n",
    "        print(f\"No sequences found in {file_path}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Always deduplicate if requested\n",
    "    if deduplicate:\n",
    "        df_before = len(df)\n",
    "        df = df.drop_duplicates(subset=['sequence'], keep='first').reset_index(drop=True)\n",
    "        print(f\"Deduplicated file. Reduced from {df_before} to {len(df)} sequences.\")\n",
    "\n",
    "    # If we do want to sort, calculate sequence identity scores\n",
    "    if do_sort and not df.empty:\n",
    "        reference_seq = df.iloc[0]['sequence']\n",
    "        reference_header = df.iloc[0]['header']\n",
    "        print(f\"Reference sequence length: {len(reference_seq)}\")\n",
    "\n",
    "        try:\n",
    "            df['similarity'] = df['sequence'].apply(\n",
    "                lambda x: calculate_sequence_identity(reference_seq, x))\n",
    "            # Set the reference's similarity to infinity so it ranks first\n",
    "            df.loc[df['header'] == reference_header, 'similarity'] = np.inf\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating similarity for {file_path}: {e}\")\n",
    "            return\n",
    "\n",
    "        # Sort in descending order\n",
    "        df = df.sort_values('similarity', ascending=False).reset_index(drop=True)\n",
    "        print(f\"Number of sequences after sorting: {len(df)}\")\n",
    "    else:\n",
    "        print(\"Skipping sorting step for this file.\")\n",
    "\n",
    "    # Save the (optionally sorted/deduplicated) sequences\n",
    "    sorted_file_path = os.path.join(output_dir, os.path.basename(file_path))\n",
    "    print(f\"Attempting to save file to: {sorted_file_path}\")\n",
    "    try:\n",
    "        with open(sorted_file_path, \"w\") as file:\n",
    "            for _, row in df.iterrows():\n",
    "                file.write(f\">{row['header']}\\n{row['sequence']}\\n\")\n",
    "        print(f\"Output saved as: {sorted_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing the file: {sorted_file_path}\")\n",
    "        print(f\"Error message: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "def main():\n",
    "    # Ask the user whether to sort 'bait_sequence.a3m'\n",
    "    sort_bait_sequence_input = input(\n",
    "        \"Warning: Sorting your a3m files might dilute coevolutionary signals.\\n\"\n",
    "        \"Do you want to sort 'bait_sequence.a3m'? (y/n): \"\n",
    "    ).strip().lower()\n",
    "    sort_bait_sequence = sort_bait_sequence_input in ['y', 'yes']\n",
    "\n",
    "    tasks = []\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".a3m\"):\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            # If it's the bait file, always deduplicate\n",
    "            if filename == \"bait_sequence.a3m\":\n",
    "                # We always deduplicate the bait\n",
    "                # Sorting is controlled by the user's choice\n",
    "                print(f\"Scheduling 'bait_sequence.a3m' with deduplication. Sorting: {sort_bait_sequence}\")\n",
    "                tasks.append((file_path, sorted_dir, True, sort_bait_sequence))\n",
    "            else:\n",
    "                # Other .a3m files remain sorted (and no dedup)\n",
    "                print(f\"Scheduling other .a3m file for sorting: {filename}\")\n",
    "                tasks.append((file_path, sorted_dir, True, True))\n",
    "\n",
    "    # Use all available CPU cores\n",
    "    num_workers = os.cpu_count() or 1\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        future_to_file = {executor.submit(sort_a3m_file, task): task for task in tasks}\n",
    "        for future in as_completed(future_to_file):\n",
    "            task = future_to_file[future]\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing {task[0]}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Trims the MSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define global parameters for maximum sequences and output directory\n",
    "MAX_SEQUENCES = 2048  # Set the maximum number of sequences to keep\n",
    "OUTPUT_DIR = truncated_a3m_output  # Define the output directory for trimmed files\n",
    "\n",
    "# Define the directory containing .a3m files\n",
    "a3m_dir = sorted_dir\n",
    "\n",
    "def trim_a3m_file(a3m_file):\n",
    "    \"\"\"\n",
    "    Trims an .a3m file to a maximum number of sequences and saves it in the OUTPUT_DIR.\n",
    "    \n",
    "    Parameters:\n",
    "    - a3m_file: Path to the original .a3m file.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "        \n",
    "    # Define the output file path\n",
    "    filename = os.path.basename(a3m_file)\n",
    "    trimmed_a3m_file = os.path.join(OUTPUT_DIR, filename)\n",
    "    \n",
    "    trimmed_lines = []\n",
    "    count = 0\n",
    "    \n",
    "    with open(a3m_file, \"r\") as file:\n",
    "        for line in file:\n",
    "            # Add headers and sequence pairs until MAX_SEQUENCES is reached\n",
    "            if line.startswith(\">\"):\n",
    "                if count >= MAX_SEQUENCES:\n",
    "                    break\n",
    "                trimmed_lines.append(line)\n",
    "                count += 1\n",
    "            else:\n",
    "                trimmed_lines.append(line)\n",
    "    \n",
    "    # Write the trimmed sequences to the output file\n",
    "    with open(trimmed_a3m_file, \"w\") as file:\n",
    "        file.writelines(trimmed_lines)\n",
    "    \n",
    "    print(f\"Trimmed {a3m_file} to {MAX_SEQUENCES} sequences in {trimmed_a3m_file}\")\n",
    "\n",
    "def process_a3m_files(a3m_dir):\n",
    "    \"\"\"\n",
    "    Trims all .a3m files in a directory to a maximum number of sequences and saves them in the OUTPUT_DIR.\n",
    "    \n",
    "    Parameters:\n",
    "    - a3m_dir: Directory containing the .a3m files.\n",
    "    \"\"\"\n",
    "    a3m_files = [os.path.join(a3m_dir, filename) for filename in os.listdir(a3m_dir) if filename.endswith(\".a3m\")]\n",
    "    \n",
    "    for a3m_file in a3m_files:\n",
    "        trim_a3m_file(a3m_file)\n",
    "\n",
    "# Trim all .a3m files in the directory\n",
    "process_a3m_files(a3m_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Combines Bait and Prey MSAs for ColabFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "# Constants for file paths\n",
    "INPUT_DIR = truncated_a3m_output\n",
    "Bait_FILE = os.path.join(INPUT_DIR, \"bait_sequence.a3m\")\n",
    "COMBINED_OUTPUT_DIR = combined_dir\n",
    "\n",
    "\n",
    "def read_a3m(file_path):\n",
    "    \"\"\"Reads an .a3m file and returns up to max_sequences headers and sequences.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    headers, sequences, seq_dict = [], [], {}\n",
    "    current_header, count = None, 0  # Initialize variables\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('>'):\n",
    "            current_header = line.strip()\n",
    "            headers.append(current_header)\n",
    "            seq_dict[current_header] = []\n",
    "            count += 1\n",
    "        elif current_header:\n",
    "            seq_dict[current_header].append(line.strip())\n",
    "\n",
    "    for header in headers:\n",
    "        sequences.append((header, ''.join(seq_dict[header])))\n",
    "\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def write_combined_a3m(file1_sequences, file2_sequences, output_file):\n",
    "    \"\"\"Combines two .a3m files with appropriate padding and custom headers.\"\"\"\n",
    "    with open(output_file, 'w') as file:\n",
    "        len1 = len(file1_sequences[0][1])\n",
    "        len2 = len(file2_sequences[0][1])\n",
    "        \n",
    "        # Combined header\n",
    "        file.write(f\"#{len1},{len2}\\t1,1\\n\")\n",
    "\n",
    "        # Write combined 101 and 102 sequence\n",
    "        file.write(f\">101\\t102\\n\")\n",
    "        combined_sequence = file1_sequences[0][1] + file2_sequences[0][1]\n",
    "        file.write(combined_sequence + '\\n')\n",
    "        \n",
    "        # Write padded 101 sequence\n",
    "        file.write(\">101\\n\")\n",
    "        file.write(file1_sequences[0][1] + '-' * len2 + '\\n')\n",
    "\n",
    "        # Write other sequences from file1\n",
    "        for header, seq in file1_sequences:\n",
    "            file.write(f\"{header}_1\\n\")\n",
    "            file.write(seq.ljust(len1 + len2, '-') + '\\n')\n",
    "\n",
    "        # Write padded 102 sequence\n",
    "        file.write(\">102\\n\")\n",
    "        file.write('-' * len1 + file2_sequences[0][1] + '\\n')\n",
    "\n",
    "        # Write other sequences from file2\n",
    "        for header, seq in file2_sequences:\n",
    "            file.write(f\"{header}_2\\n\")\n",
    "            file.write('-' * len1 + seq + '\\n')\n",
    "\n",
    "\n",
    "def process_single_file(file_name, bait_sequences):\n",
    "    \"\"\"Processes a single .a3m file by combining it with the actin sequences.\"\"\"\n",
    "    input_file_path = os.path.join(INPUT_DIR, file_name)\n",
    "    output_file_path = os.path.join(COMBINED_OUTPUT_DIR, f\"c_{file_name}\")\n",
    "\n",
    "    input_sequences = read_a3m(input_file_path)\n",
    "    write_combined_a3m(input_sequences, bait_sequences, output_file_path)\n",
    "    print(f\"Combined {file_name} with actin.a3m into {os.path.basename(output_file_path)}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(COMBINED_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # Read actin sequences once\n",
    "    bait_sequences = read_a3m(Bait_FILE)\n",
    "\n",
    "    # List all input files\n",
    "    file_names = [f for f in os.listdir(INPUT_DIR) if f.endswith(\".a3m\")]\n",
    "\n",
    "    # Multiprocessing to process files in parallel\n",
    "    num_cores = multiprocessing.cpu_count()  # Adjust based on system\n",
    "    with multiprocessing.Pool(processes=num_cores) as pool:\n",
    "        pool.starmap(process_single_file, [(file_name, bait_sequences) for file_name in file_names])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SLiMFold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
