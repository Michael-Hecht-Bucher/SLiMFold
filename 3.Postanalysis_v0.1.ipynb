{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Folder and pathway setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Define and import\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Path to the folder containing the .zip files\n",
    "zip_files_folder = #\"/home/.../Test/Output/AF2_Results/zip_files\"# Replace with your actual path\n",
    "\n",
    "# Path to the folder containing the FASTA filesf\n",
    "fasta_folder = #\"/home/.../Test/Output/Fasta/\"# Replace with your actual path\n",
    "\n",
    "# Path to the reference PDB file\n",
    "reference_pdb_path = #\"/home/.../Test/Output/AF2_Results/zip_files/reference.pdb\"# Update this path\n",
    "\n",
    "# Path to the output folder\n",
    "output_directory = #\"/home/.../Test/Output/AF2_Results\"# Directory for output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Unpacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import zipfile\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def extract_zip_file(zip_file, zip_files_folder):\n",
    "    zip_file_path = os.path.join(zip_files_folder, zip_file)\n",
    "    # Extract folder name\n",
    "    extract_folder_name = os.path.splitext(zip_file)[0]\n",
    "    extract_folder_path = os.path.join(zip_files_folder, extract_folder_name)\n",
    "    \n",
    "    # Check if the folder already exists\n",
    "    if os.path.exists(extract_folder_path):\n",
    "        print(f\"Skipping {zip_file} as it is already extracted.\")\n",
    "        return\n",
    "    \n",
    "    # Extract the zip file\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_folder_path)\n",
    "        print(f\"Extracted {zip_file} to {extract_folder_path}\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"Error: {zip_file} is not a valid zip file.\")\n",
    "\n",
    "def parallel_extract(zip_files_folder):\n",
    "    # List all .zip files in the specified folder\n",
    "    zip_files = [f for f in os.listdir(zip_files_folder) if f.endswith('.zip')]\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        executor.map(lambda zip_file: extract_zip_file(zip_file, zip_files_folder), zip_files)\n",
    "\n",
    "# Specify the folder containing the .zip files\n",
    "parallel_extract(zip_files_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Analysis of Model Metrics and Structural Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This script processes computational models generated in the pipeline, focusing on evaluating their structural properties. It extracts key metrics such as RMSD (Root Mean Square Deviation), spherical angles (theta and phi), and helix polarity by comparing the predicted models against a reference structure. Additionally, it computes average model scores for pLDDT, pTM, and ipTM to assess the overall quality of the predictions.\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np  # For handling numerical calculations\n",
    "from Bio.PDB import PDBParser, Superimposer\n",
    "from Bio.PDB.Polypeptide import is_aa\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set the number of top models to consider for metrics (e.g., 1, 3, or 5)\n",
    "top_n_models_metrics = 3  # For pLDDT, pTM, ipTM\n",
    "\n",
    "# Set the number of top models to consider for RMSD and angle calculations\n",
    "top_n_models_rmsd = 1  # Usually 1\n",
    "\n",
    "# Path to the folder containing the unzipped result folders\n",
    "unzipped_results_folder = zip_files_folder  # Update this path\n",
    "\n",
    "# List all unzipped result folders in the specified folder\n",
    "result_folders = [\n",
    "    f for f in os.listdir(unzipped_results_folder)\n",
    "    if os.path.isdir(os.path.join(unzipped_results_folder, f))\n",
    "]\n",
    "\n",
    "# Initialize a list to store combined results\n",
    "all_combined_results = []\n",
    "\n",
    "# Define helper functions for calculations\n",
    "def calculate_helix_axis(ca_atoms):\n",
    "    coords = np.array([atom.get_coord() for atom in ca_atoms])\n",
    "    if coords.shape[0] < 2:\n",
    "        print(\"Not enough atoms to define helix axis.\")\n",
    "        return None, None, None, None\n",
    "    # Get the mean (center) of the coordinates\n",
    "    center = coords.mean(axis=0)\n",
    "    # Subtract the mean to center the data\n",
    "    centered_coords = coords - center\n",
    "    # Perform SVD\n",
    "    U, s, Vh = np.linalg.svd(centered_coords)\n",
    "    # The principal component is the first row of Vh\n",
    "    axis = Vh[0]\n",
    "    # Ensure that the axis is a unit vector\n",
    "    axis /= np.linalg.norm(axis)\n",
    "    # Ensure the direction points from first to last CA atom\n",
    "    if np.dot(axis, coords[-1] - coords[0]) < 0:\n",
    "        axis = -axis\n",
    "    # For visualization, compute the start and end points along the axis\n",
    "    projections = np.dot(centered_coords, axis)\n",
    "    min_proj = projections.min()\n",
    "    max_proj = projections.max()\n",
    "    # The start point is center + min projection * axis\n",
    "    start_point = center + axis * min_proj\n",
    "    # The end point is center + max projection * axis\n",
    "    end_point = center + axis * max_proj\n",
    "    # Print start and end coordinates\n",
    "    print(f\"Start point of axis: {start_point}\")\n",
    "    print(f\"End point of axis: {end_point}\")\n",
    "    return axis, center, start_point, end_point\n",
    "\n",
    "def calculate_spherical_angles(axis):\n",
    "    # Ensure the axis is a unit vector\n",
    "    axis = axis / np.linalg.norm(axis)\n",
    "    v_x, v_y, v_z = axis\n",
    "    # Compute theta (polar angle)\n",
    "    theta = np.arccos(v_z)  # Range [0, pi]\n",
    "    # Compute phi (azimuthal angle)\n",
    "    phi = np.arctan2(v_y, v_x)  # Range [-pi, pi]\n",
    "    # Convert angles to degrees\n",
    "    theta_deg = np.degrees(theta)\n",
    "    phi_deg = np.degrees(phi)\n",
    "    # Ensure phi is in the range [0, 360)\n",
    "    if phi_deg < 0:\n",
    "        phi_deg += 360\n",
    "    return theta_deg, phi_deg\n",
    "\n",
    "def calculate_helix_polarity(ref_axis, current_axis):\n",
    "    # Ensure the axes are unit vectors\n",
    "    ref_axis = ref_axis / np.linalg.norm(ref_axis)\n",
    "    current_axis = current_axis / np.linalg.norm(current_axis)\n",
    "    # Compute the dot product\n",
    "    dot_product = np.dot(ref_axis, current_axis)\n",
    "    # Determine polarity (+1 or -1)\n",
    "    polarity = np.sign(dot_product)\n",
    "    return polarity\n",
    "\n",
    "def calculate_angle_between_axes(axis1, axis2):\n",
    "    # Ensure the axes are unit vectors\n",
    "    axis1 = axis1 / np.linalg.norm(axis1)\n",
    "    axis2 = axis2 / np.linalg.norm(axis2)\n",
    "    # Compute the dot product without taking the absolute value\n",
    "    dot_product = np.clip(np.dot(axis1, axis2), -1.0, 1.0)\n",
    "    angle_rad = np.arccos(dot_product)\n",
    "    angle_deg = np.degrees(angle_rad)\n",
    "    return angle_deg\n",
    "\n",
    "def adjust_angle_difference(angle1, angle2):\n",
    "    delta = angle2 - angle1\n",
    "    # Adjust delta to be within -180° to 180°\n",
    "    delta = (delta + 180) % 360 - 180\n",
    "    return delta\n",
    "\n",
    "# Read the reference PDB file\n",
    "parser = PDBParser(QUIET=True)\n",
    "ref_structure = parser.get_structure('reference', reference_pdb_path)\n",
    "ref_model = ref_structure[0]  # Assuming we use model 0\n",
    "ref_chainA = ref_model['A']\n",
    "ref_chainB = ref_model['B']\n",
    "\n",
    "# Process each unzipped result folder\n",
    "for folder_name in result_folders:\n",
    "    folder_path = os.path.join(unzipped_results_folder, folder_name)\n",
    "\n",
    "    # Find the subfolder that contains 'log.txt'\n",
    "    subfolders = [f.name for f in os.scandir(folder_path) if f.is_dir()]\n",
    "    if subfolders:\n",
    "        # Assuming there's only one subfolder\n",
    "        subfolder_name = subfolders[0]\n",
    "        subfolder_path = os.path.join(folder_path, subfolder_name)\n",
    "    else:\n",
    "        print(f\"No subfolders found in {folder_path}\")\n",
    "        continue\n",
    "\n",
    "    # Path to the log.txt file inside the subfolder\n",
    "    log_file_path = os.path.join(subfolder_path, 'log.txt')\n",
    "\n",
    "    # Check if log.txt exists\n",
    "    if not os.path.exists(log_file_path):\n",
    "        print(f\"log.txt not found in {subfolder_path}\")\n",
    "        continue\n",
    "\n",
    "    # Initialize lists to store extracted data\n",
    "    model_names = []\n",
    "    pLDDT_values = []\n",
    "    pTM_values = []\n",
    "    ipTM_values = []\n",
    "\n",
    "    # Read the log.txt file\n",
    "    with open(log_file_path, 'r') as log_file:\n",
    "        lines = log_file.readlines()\n",
    "\n",
    "    # Flag to indicate the start of the reranking section\n",
    "    rerank_section = False\n",
    "\n",
    "    for line in lines:\n",
    "        if \"reranking models by 'multimer' metric\" in line:\n",
    "            rerank_section = True\n",
    "            continue\n",
    "        if rerank_section:\n",
    "            if line.strip() == '':\n",
    "                break  # End of the reranking section\n",
    "            else:\n",
    "                # Extract model name and metrics\n",
    "                match = re.match(\n",
    "                    r'.*rank_\\d+_(\\S+) pLDDT=([\\d\\.]+) pTM=([\\d\\.]+) ipTM=([\\d\\.]+)', line)\n",
    "                if match:\n",
    "                    model_names.append(match.group(1))\n",
    "                    pLDDT_values.append(float(match.group(2)))\n",
    "                    pTM_values.append(float(match.group(3)))\n",
    "                    ipTM_values.append(float(match.group(4)))\n",
    "\n",
    "    # Create a DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'Model': model_names,\n",
    "        'pLDDT': pLDDT_values,\n",
    "        'pTM': pTM_values,\n",
    "        'ipTM': ipTM_values\n",
    "    })\n",
    "\n",
    "    if data.empty:\n",
    "        print(f\"No data found in {log_file_path}\")\n",
    "        continue\n",
    "\n",
    "    # Select the top N models for metrics\n",
    "    top_models_metrics = data.head(top_n_models_metrics)\n",
    "\n",
    "    # Compute mean values for metrics\n",
    "    mean_pLDDT = top_models_metrics['pLDDT'].mean()\n",
    "    mean_pTM = top_models_metrics['pTM'].mean()\n",
    "    mean_ipTM = top_models_metrics['ipTM'].mean()\n",
    "\n",
    "    # Display the top models and mean values\n",
    "    print(f\"\\nProcessing folder: {folder_name}\")\n",
    "\n",
    "    # Save the top models to a CSV file in the subfolder\n",
    "    output_csv_path = os.path.join(subfolder_path, 'top_models_metrics.csv')\n",
    "    top_models_metrics.to_csv(output_csv_path, index=False)\n",
    "    print(f\"\\nResults saved to {output_csv_path}\")\n",
    "\n",
    "    # Get the base name to match with the FASTA file\n",
    "    # Extract accession and position\n",
    "    base_name_match = re.match(r'([A-Z]{2}_\\d+)(\\d)_pos_(\\d+)', folder_name)\n",
    "    if base_name_match:\n",
    "        accession_without_version = base_name_match.group(1)\n",
    "        version_number = base_name_match.group(2)\n",
    "        position = base_name_match.group(3)\n",
    "        # Reconstruct the accession with the dot before the version number\n",
    "        accession_with_version = f\"{accession_without_version}.{version_number}\"\n",
    "        base_name = f\"{accession_with_version}_pos_{position}\"\n",
    "    else:\n",
    "        # Handle the case where the folder name might differ\n",
    "        base_name = folder_name  # Adjust as needed\n",
    "\n",
    "    # Path to the corresponding FASTA file\n",
    "    fasta_file_path = os.path.join(fasta_folder, f'{base_name}.fasta')\n",
    "\n",
    "    # Initialize fasta_info dictionary\n",
    "    fasta_info = {\n",
    "        'name': 'N/A',\n",
    "        'position': 'N/A',\n",
    "        'pssm_score': np.nan,\n",
    "        'iupred_score': np.nan,\n",
    "        'anchor_score': np.nan,\n",
    "        'coil_score': np.nan,\n",
    "        'helix_score': np.nan,\n",
    "        'strand_score': np.nan\n",
    "    }\n",
    "\n",
    "    # Check if the FASTA file exists\n",
    "    if not os.path.exists(fasta_file_path):\n",
    "        print(f\"FASTA file not found for {base_name}\")\n",
    "    else:\n",
    "        # Read the FASTA file\n",
    "        with open(fasta_file_path, 'r') as fasta_file:\n",
    "            fasta_header = fasta_file.readline().strip()\n",
    "            sequence = fasta_file.readline().strip()\n",
    "\n",
    "        # Extract information from the FASTA header\n",
    "        header_pattern = (\n",
    "            r'>(?P<name>.+?) '\n",
    "            r'pos:(?P<position>\\d+) '\n",
    "            r'PSSM_Score:(?P<pssm_score>-?[\\d\\.]+) '\n",
    "            r'IUPRED_Score:(?P<iupred_score>-?[\\d\\.]+) '\n",
    "            r'ANCHOR_Score:(?P<anchor_score>-?[\\d\\.]+) '\n",
    "            r'Coil:(?P<coil_score>-?[\\d\\.]+) '\n",
    "            r'Helix:(?P<helix_score>-?[\\d\\.]+) '\n",
    "            r'Strand:(?P<strand_score>-?[\\d\\.]+)'\n",
    "        )\n",
    "\n",
    "        header_match = re.match(header_pattern, fasta_header)\n",
    "        if header_match:\n",
    "            fasta_info = header_match.groupdict()\n",
    "            # Convert numerical values to float\n",
    "            for key in [\n",
    "                    'pssm_score', 'iupred_score', 'anchor_score',\n",
    "                    'coil_score', 'helix_score', 'strand_score']:\n",
    "                fasta_info[key] = float(fasta_info[key])\n",
    "        else:\n",
    "            print(f\"Failed to parse FASTA header for {base_name}\")\n",
    "\n",
    "    # Display the extracted FASTA information\n",
    "    print(\"\\nExtracted FASTA Information:\")\n",
    "    for key, value in fasta_info.items():\n",
    "        print(f\"{key.capitalize()}: {value}\")\n",
    "\n",
    "    # Now, calculate RMSD and angles for the top N models specified\n",
    "    rmsd_values = []\n",
    "    delta_theta_values = []\n",
    "    delta_phi_values = []\n",
    "    helix_polarity_values = []\n",
    "\n",
    "    # Select the top N models for RMSD and angle calculations\n",
    "    top_models_rmsd = data.head(top_n_models_rmsd)\n",
    "\n",
    "    for i, row in top_models_rmsd.iterrows():\n",
    "        model_name = row['Model']\n",
    "        # Find the PDB file corresponding to this model\n",
    "        pdb_file = None\n",
    "        for file_name in os.listdir(subfolder_path):\n",
    "            if model_name in file_name and file_name.endswith('.pdb'):\n",
    "                pdb_file = os.path.join(subfolder_path, file_name)\n",
    "                break\n",
    "        if not pdb_file:\n",
    "            print(f\"PDB file for model {model_name} not found in {subfolder_path}\")\n",
    "            continue\n",
    "\n",
    "        # Extract 'pos' number from the filename\n",
    "        match = re.search(r'_pos_(\\d+)_', os.path.basename(pdb_file))\n",
    "        if match:\n",
    "            pos_number = int(match.group(1))\n",
    "        else:\n",
    "            print(f\"Could not find 'pos' number in {pdb_file}\")\n",
    "            continue\n",
    "\n",
    "        # Read the PDB file\n",
    "        try:\n",
    "            structure = parser.get_structure('current', pdb_file)\n",
    "            model = structure[0]\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDB file {pdb_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Extract chains\n",
    "        try:\n",
    "            chainA = model['A']\n",
    "            chainB = model['B']\n",
    "        except KeyError as e:\n",
    "            print(f\"Chain {e} not found in {pdb_file}\")\n",
    "            continue\n",
    "\n",
    "        # Get CA atoms from chain B for alignment\n",
    "        ref_atoms = []\n",
    "        current_atoms = []\n",
    "        for ref_res, cur_res in zip(ref_chainB, chainB):\n",
    "            if is_aa(ref_res) and is_aa(cur_res):\n",
    "                try:\n",
    "                    ref_atoms.append(ref_res['CA'])\n",
    "                    current_atoms.append(cur_res['CA'])\n",
    "                except KeyError:\n",
    "                    pass  # Skip residues that don't have CA atom\n",
    "\n",
    "        if len(ref_atoms) == 0 or len(current_atoms) == 0:\n",
    "            print(f\"No CA atoms found for alignment in chain B of {pdb_file}\")\n",
    "            continue\n",
    "\n",
    "        # Align chain B\n",
    "        super_imposer = Superimposer()\n",
    "        super_imposer.set_atoms(ref_atoms, current_atoms)\n",
    "        super_imposer.apply(model.get_atoms())  # Apply transformation to all atoms in the model\n",
    "\n",
    "        # Determine residue ranges\n",
    "        flanking_size = 20\n",
    "        motif_size = 9\n",
    "        ref_start_res = flanking_size + 1\n",
    "        ref_end_res = ref_start_res + motif_size - 1\n",
    "        if pos_number >= ref_start_res:\n",
    "            current_start_res = ref_start_res\n",
    "            current_end_res = ref_end_res\n",
    "        else:\n",
    "            current_start_res = pos_number\n",
    "            current_end_res = pos_number + motif_size - 1\n",
    "\n",
    "        # Get CA atoms from chain A residues for RMSD calculation\n",
    "        ref_atoms = []\n",
    "        current_atoms = []\n",
    "        # For the reference chain A\n",
    "        for res_id in range(ref_start_res, ref_end_res + 1):\n",
    "            try:\n",
    "                ref_res = ref_chainA[(' ', res_id, ' ')]\n",
    "                if is_aa(ref_res):\n",
    "                    ref_atoms.append(ref_res['CA'])\n",
    "            except KeyError:\n",
    "                print(f\"Residue {res_id} not found in reference chain A\")\n",
    "        # For the current chain A\n",
    "        for res_id in range(current_start_res, current_end_res + 1):\n",
    "            try:\n",
    "                current_res = chainA[(' ', res_id, ' ')]\n",
    "                if is_aa(current_res):\n",
    "                    current_atoms.append(current_res['CA'])\n",
    "            except KeyError:\n",
    "                print(f\"Residue {res_id} not found in chain A of {pdb_file}\")\n",
    "\n",
    "        if len(ref_atoms) != len(current_atoms) or len(ref_atoms) == 0:\n",
    "            print(f\"Number of atoms does not match or zero atoms found for {pdb_file}\")\n",
    "            continue\n",
    "\n",
    "        # Calculate RMSD\n",
    "        ref_coords = np.array([atom.get_coord() for atom in ref_atoms])\n",
    "        current_coords = np.array([atom.get_coord() for atom in current_atoms])\n",
    "        diff = ref_coords - current_coords\n",
    "        rmsd = np.sqrt(np.mean(np.sum(diff * diff, axis=1)))\n",
    "        rmsd_values.append(rmsd)\n",
    "        print(f\"RMSD for model {model_name} (pos {pos_number}): {rmsd:.3f}\")\n",
    "\n",
    "        # Extract helix atoms for helix axis calculation\n",
    "        # For the reference chain A\n",
    "        ref_helix_atoms = []\n",
    "        for res_id in range(ref_start_res, ref_end_res + 1):\n",
    "            try:\n",
    "                ref_res = ref_chainA[(' ', res_id, ' ')]\n",
    "                if is_aa(ref_res):\n",
    "                    ref_helix_atoms.append(ref_res['CA'])\n",
    "            except KeyError:\n",
    "                print(f\"Residue {res_id} not found in reference chain A for helix axis calculation\")\n",
    "\n",
    "        # For the current chain A\n",
    "        current_helix_atoms = []\n",
    "        for res_id in range(current_start_res, current_end_res + 1):\n",
    "            try:\n",
    "                current_res = chainA[(' ', res_id, ' ')]\n",
    "                if is_aa(current_res):\n",
    "                    current_helix_atoms.append(current_res['CA'])\n",
    "            except KeyError:\n",
    "                print(f\"Residue {res_id} not found in chain A of {pdb_file} for helix axis calculation\")\n",
    "\n",
    "        # Calculate helix axes and angles\n",
    "        if len(ref_helix_atoms) >= 2 and len(current_helix_atoms) >= 2:\n",
    "            ref_axis, ref_center, ref_start_point, ref_end_point = calculate_helix_axis(ref_helix_atoms)\n",
    "            current_axis, current_center, current_start_point, current_end_point = calculate_helix_axis(current_helix_atoms)\n",
    "            if ref_axis is None or current_axis is None:\n",
    "                print(f\"Could not calculate helix axis for {pdb_file}\")\n",
    "                continue\n",
    "            # Calculate the angle between the helix axes\n",
    "            helix_angle = calculate_angle_between_axes(ref_axis, current_axis)\n",
    "\n",
    "            # Calculate helix polarity\n",
    "            helix_polarity = calculate_helix_polarity(ref_axis, current_axis)\n",
    "            helix_polarity_values.append(helix_polarity)\n",
    "\n",
    "            # Calculate spherical angles for the reference helix\n",
    "            ref_theta, ref_phi = calculate_spherical_angles(ref_axis)\n",
    "            # Calculate spherical angles for the current helix\n",
    "            current_theta, current_phi = calculate_spherical_angles(current_axis)\n",
    "            # Calculate adjusted angle differences\n",
    "            delta_theta = adjust_angle_difference(ref_theta, current_theta)\n",
    "            delta_phi = adjust_angle_difference(ref_phi, current_phi)\n",
    "            #Store the spherical angle differences\n",
    "            delta_theta_values.append(delta_theta)\n",
    "            delta_phi_values.append(delta_phi)\n",
    "\n",
    "    # After processing all models, calculate mean values\n",
    "    if len(rmsd_values) > 0:\n",
    "        mean_rmsd = np.mean(rmsd_values)\n",
    "    else:\n",
    "        mean_rmsd = np.nan\n",
    "    \n",
    "\n",
    "    if len(delta_theta_values) > 0:\n",
    "        mean_delta_theta = np.nanmean(delta_theta_values)\n",
    "        mean_delta_phi = np.nanmean(delta_phi_values)\n",
    "        mean_helix_polarity = np.nanmean(helix_polarity_values)\n",
    "    else:\n",
    "        mean_delta_theta = mean_delta_phi = np.nan\n",
    "        \n",
    "    # Combine all data into a single dictionary\n",
    "    combined_data = {\n",
    "        'Folder': folder_name,\n",
    "        'Name': fasta_info.get('name', 'N/A'),\n",
    "        'Position': fasta_info.get('position', 'N/A'),\n",
    "        'PSSM_Score': fasta_info.get('pssm_score', np.nan),\n",
    "        'IUPRED_Score': fasta_info.get('iupred_score', np.nan),\n",
    "        'ANCHOR_Score': fasta_info.get('anchor_score', np.nan),\n",
    "        'Coil_Score': fasta_info.get('coil_score', np.nan),\n",
    "        'Helix_Score': fasta_info.get('helix_score', np.nan),\n",
    "        'Strand_Score': fasta_info.get('strand_score', np.nan),\n",
    "        'Mean_pLDDT': mean_pLDDT,\n",
    "        'Mean_pTM': mean_pTM,\n",
    "        'Mean_ipTM': mean_ipTM,\n",
    "        'Mean_RMSD': mean_rmsd,\n",
    "        'Mean_Delta_Theta': mean_delta_theta,\n",
    "        'Mean_Delta_Phi': mean_delta_phi,\n",
    "        'Mean_Helix_Polarity': mean_helix_polarity,\n",
    "    }\n",
    "\n",
    "\n",
    "    # Add to the list of combined results\n",
    "    all_combined_results.append(combined_data)\n",
    "\n",
    "    # Save combined results for this folder in the subfolder\n",
    "    combined_df = pd.DataFrame([combined_data])\n",
    "    output_combined_csv_path = os.path.join(subfolder_path, 'combined_results.csv')\n",
    "    combined_df.to_csv(output_combined_csv_path, index=False)\n",
    "    print(f\"Combined results saved to {output_combined_csv_path}\")\n",
    "\n",
    "# After processing all folders, save all combined results into a single CSV file\n",
    "if all_combined_results:\n",
    "    all_combined_df = pd.DataFrame(all_combined_results)\n",
    "\n",
    "    # Save to CSV in the main results folder\n",
    "    output_all_combined_csv_path = os.path.normpath(\n",
    "        os.path.join(output_directory, 'all_combined_results.csv')\n",
    "    )\n",
    "    all_combined_df.to_csv(output_all_combined_csv_path, index=False)\n",
    "    print(f\"\\nAll combined results saved to {output_all_combined_csv_path}\")\n",
    "\n",
    "    # Optionally display the combined DataFrame\n",
    "    # print(all_combined_df)\n",
    "else:\n",
    "    print(\"\\nNo combined results to save.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Filter Combined Results by ipTM Cutoff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Reset all matplotlib settings to their default values\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "\n",
    "\n",
    "# Set the path to the input CSV file and the cutoff for ipTM\n",
    "input_csv_path = output_all_combined_csv_path  # Input CSV\n",
    "output_file_name = 'all_combined_results_ipTM_Cutoff.csv'  # Desired output file name\n",
    "output_filtered_csv_path = os.path.join(output_directory, output_file_name)  # Combine directory and file name\n",
    "\n",
    "iptm_cutoff = 0.6  # Desired ipTM cutoff value\n",
    "\n",
    "# Read the combined results CSV\n",
    "try:\n",
    "    combined_results_df = pd.read_csv(input_csv_path)\n",
    "    print(f\"Successfully read the CSV file: {input_csv_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {input_csv_path}. Please check the path and try again.\")\n",
    "    exit()\n",
    "\n",
    "def create_scatter_plot(df, title, output_path, iptm_col='Mean_ipTM', rmsd_col='Mean_RMSD'):\n",
    "    # Extract relevant columns for the scatter plot\n",
    "    iptm_values = df[iptm_col]\n",
    "    rmsd_values = df[rmsd_col]  # Use normal RMSD values\n",
    "    \n",
    "    # Create the figure and axis explicitly\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Create scatter plot with black dots\n",
    "    ax.scatter(iptm_values, rmsd_values, c='black', s=40, alpha=1)\n",
    "\n",
    "    # Add a vertical red dashed line at ipTM cutoff (0.6)\n",
    "    ax.axvline(x=0.6, color='red', linestyle=(0, (5, 5)), linewidth=1.5, label='ipTM Cutoff (0.6)')\n",
    "\n",
    "    # Adjust tick size and font\n",
    "    ax.tick_params(axis='x', which='major', length=8, labelsize=14)\n",
    "    ax.tick_params(axis='y', which='major', length=8, labelsize=14)\n",
    "\n",
    "    # Adjust graph line thickness and set spine colors to black\n",
    "    ax.spines['top'].set_linewidth(1.2)\n",
    "    ax.spines['top'].set_color('black')\n",
    "    ax.spines['right'].set_linewidth(1.2)\n",
    "    ax.spines['right'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.2)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1.2)\n",
    "    ax.spines['left'].set_color('black')\n",
    "\n",
    "    # Add labels and legend\n",
    "    ax.set_xlabel('Mean ipTM', fontsize=15, fontweight='bold')\n",
    "    ax.set_ylabel('RMSD (Å)', fontsize=15, fontweight='bold')\n",
    "    ax.legend(fontsize=12)\n",
    "\n",
    "    # Save the figure in EPS format\n",
    "    try:\n",
    "        fig.savefig(output_path, format='eps', dpi=300, bbox_inches='tight')\n",
    "        print(f\"Scatter plot saved as EPS to {output_path}\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}\")\n",
    "        print(\"Ensure the output path is correct and writable.\")\n",
    "\n",
    "    # Also save the figure in TIFF format\n",
    "    tiff_output_path = output_path.replace('.eps', '.tif')\n",
    "    try:\n",
    "        fig.savefig(tiff_output_path, format='tiff', dpi=300, bbox_inches='tight')\n",
    "        print(f\"Scatter plot saved as TIFF to {tiff_output_path}\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}\")\n",
    "        print(\"Ensure the output path is correct and writable.\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot full dataset before filtering\n",
    "plot_title_full = \"SLiMFold Predictions: ipTM vs RMSD Scatter Plot\"\n",
    "output_plot_full_path = os.path.join(output_directory, 'scatter_ipTM_vs_RMSD_Full.eps')  # Save as .eps\n",
    "create_scatter_plot(combined_results_df, plot_title_full, output_plot_full_path)\n",
    "\n",
    "# Filter the DataFrame based on the ipTM cutoff\n",
    "filtered_results_df = combined_results_df[combined_results_df['Mean_ipTM'] >= iptm_cutoff]\n",
    "\n",
    "# Save the filtered results to a new CSV file\n",
    "try:\n",
    "    filtered_results_df.to_csv(output_filtered_csv_path, index=False)\n",
    "    print(f\"Filtered results saved to {output_filtered_csv_path}\")\n",
    "except PermissionError as e:\n",
    "    print(f\"PermissionError: {e}\")\n",
    "    print(\"Ensure the output path is correct and writable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 5. Visualization of 2D and 3D Scatter Plots for Protein Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Reset all matplotlib settings to their default values\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = output_filtered_csv_path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract relevant columns\n",
    "mean_delta_theta = data['Mean_Delta_Theta']\n",
    "mean_delta_phi = data['Mean_Delta_Phi']\n",
    "mean_helix_polarity = data['Mean_Helix_Polarity']\n",
    "mean_rmsd = np.log1p(data['Mean_RMSD'])\n",
    "\n",
    "\n",
    "\n",
    "# Define a green-to-red colormap\n",
    "blue_white_red = LinearSegmentedColormap.from_list('BlueWhiteRed', ['blue', 'white', 'red'])\n",
    "\n",
    "# Create a figure\n",
    "fig = plt.figure(figsize=(18, 7))  # Increase the figure size\n",
    "fig.suptitle('Conformational Landscape: RMSD Variation Across Angular and Polarity Dimensions', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 3D Scatter Plot\n",
    "ax3d = fig.add_subplot(1, 2, 1, projection='3d')  # Second column\n",
    "scatter3d = ax3d.scatter(\n",
    "    mean_delta_theta, mean_delta_phi, mean_helix_polarity, \n",
    "    c=mean_rmsd, s=40, cmap=blue_white_red, edgecolors='black', linewidths=0.5\n",
    ")\n",
    "\n",
    "\n",
    "# Adjust the ticks for the axes\n",
    "ax3d.set_zticks([-1, 0, 1])  # Helix polarity axis: only +1 and -1\n",
    "ax3d.set_yticks(np.arange(-200, 201, 100))  # Delta Phi axis: ticks in -100 steps\n",
    "\n",
    "# Adjust the thickness of the gridlines and axis lines\n",
    "ax3d.tick_params(axis='x', colors='black', width=1.5)\n",
    "ax3d.tick_params(axis='y', colors='black', width=1.5)\n",
    "ax3d.tick_params(axis='z', colors='black', width=1.5)\n",
    "\n",
    "\n",
    "# Make axis lines black and thicker\n",
    "ax3d.xaxis._axinfo[\"axisline\"][\"color\"] = (0,0,0,1)  # Black\n",
    "ax3d.xaxis._axinfo[\"axisline\"][\"linewidth\"] = 2.0\n",
    "\n",
    "ax3d.yaxis._axinfo[\"axisline\"][\"color\"] = (0,0,0,1)  # Black\n",
    "ax3d.yaxis._axinfo[\"axisline\"][\"linewidth\"] = 2.0\n",
    "\n",
    "ax3d.zaxis._axinfo[\"axisline\"][\"color\"] = (0,0,0,1)  # Black\n",
    "ax3d.zaxis._axinfo[\"axisline\"][\"linewidth\"] = 2.0\n",
    "\n",
    "\n",
    "# Adjust the line width of the grid and axis lines\n",
    "ax3d.grid(True, color='black', linewidth=1.5)\n",
    "\n",
    "# Set labels, grid, and limits\n",
    "ax3d.set_xlabel(r'$\\boldsymbol{\\Delta\\theta} \\, \\boldsymbol{(°)}$', fontsize=13, fontweight='bold')\n",
    "ax3d.set_ylabel(r'$\\boldsymbol{\\Delta\\phi} \\, \\boldsymbol{(°)}$', fontsize=13, fontweight='bold')\n",
    "ax3d.set_zlabel('Helix Polarity', fontsize=12, fontweight='bold')\n",
    "ax3d.set_title('3D Conformational Space (ΔΘ, ΔΦ, Polarity)', fontsize=14, fontweight='bold', x=0.6)\n",
    "\n",
    "# Add gridlines and axis limits\n",
    "ax3d.grid(color='gray', linestyle='--', linewidth=0.3)\n",
    "\n",
    "# Adjust tick size and font\n",
    "ax3d.tick_params(axis='both', which='major', length=8, width=1.5, labelsize=10)  # Major ticks\n",
    "ax3d.tick_params(axis='both', which='minor', length=4, width=1.0, labelsize=8)  # Minor ticks (optional)\n",
    "\n",
    "# Line for Theta = 0 (X = 0, spans full Y and Z range)\n",
    "ax3d.plot([0, 0], [-200, 200], [1.0, 1.0], color='black', linestyle=(0, (5, 5)), linewidth=1.2)\n",
    "\n",
    "# Line for Phi = 0 (Y = 0, spans full X and Z range)\n",
    "ax3d.plot([-80, 40], [0, 0], [1.0, 1.0], color='black', linestyle=(0, (5, 5)), linewidth=1.2)\n",
    "\n",
    "# Line for Polarity = 1 (Z = 1, spans full X and Y range)\n",
    "ax3d.plot([0, 0], [0, 0], [-1, 1.2], color='black', linestyle=(0, (5, 5)), linewidth=1.2)\n",
    "\n",
    "# Add color bar for 3D plot\n",
    "cbar3d = fig.colorbar(scatter3d, ax=ax3d, shrink=0.6, aspect=25, pad=0.1)  # Adjust shrink to make the 3D bar consistent\n",
    "cbar3d.set_label(\"log(1 + RMSD) (Å)\", fontsize=10, fontweight='bold')\n",
    "cbar3d.ax.tick_params(width=1)  # Make color bar ticks thicker\n",
    "cbar3d.outline.set_linewidth(1) \n",
    "cbar3d.outline.set_edgecolor('black')  # Adjust color bar outline thickness\n",
    "\n",
    "# 2D Scatter Plot\n",
    "ax2d = fig.add_subplot(1, 2, 2)  # First column\n",
    "scatter2d = ax2d.scatter(\n",
    "    mean_delta_theta, mean_delta_phi,\n",
    "    c=mean_rmsd, s=40, cmap=blue_white_red, edgecolors='black', linewidths=0.5\n",
    ")\n",
    "ax2d.set_xlabel(r'$\\boldsymbol{\\Delta\\theta} \\, \\boldsymbol{(°)}$', fontsize=13, fontweight='bold')\n",
    "ax2d.set_ylabel(r'$\\boldsymbol{\\Delta\\phi} \\, \\boldsymbol{(°)}$', fontsize=13, fontweight='bold')\n",
    "ax2d.set_title('2D Angular Variation (ΔΘ vs ΔΦ) with RMSD Coloring', fontsize=14, fontweight='bold')\n",
    "ax2d.grid(color='gray', linestyle='--', linewidth=0.3)\n",
    "\n",
    "ax2d.axhline(0, color='black', linestyle=(0, (5, 5)), linewidth=1.2)  # Dashed Phi=0 line\n",
    "ax2d.axvline(0, color='black', linestyle=(0, (5, 5)), linewidth=1.2)  # Dashed Theta=0 line\n",
    "\n",
    "# Adjust tick size and font\n",
    "ax2d.tick_params(axis='both', which='major', length=8, width=1.5, labelsize=12)  # Major ticks\n",
    "ax2d.tick_params(axis='both', which='minor', length=4, width=1.0, labelsize=10)  # Minor ticks (optional)\n",
    "\n",
    "# Adjust graph line thickness and set spine colors to black\n",
    "ax2d.spines['top'].set_linewidth(1.5)\n",
    "ax2d.spines['top'].set_color('black')\n",
    "ax2d.spines['right'].set_linewidth(1.5)\n",
    "ax2d.spines['right'].set_color('black')\n",
    "ax2d.spines['bottom'].set_linewidth(1.5)\n",
    "ax2d.spines['bottom'].set_color('black')\n",
    "ax2d.spines['left'].set_linewidth(1.5)\n",
    "ax2d.spines['left'].set_color('black')\n",
    "\n",
    "\n",
    "# Add color bar for 2D plot\n",
    "cbar2d = fig.colorbar(scatter2d, ax=ax2d, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "cbar2d.set_label(\"log(1 + RMSD) (Å)\", fontsize=10, fontweight='bold')\n",
    "cbar2d.ax.tick_params(width=1.5)  # Make color bar ticks thicker\n",
    "cbar2d.outline.set_linewidth(1.5)  # Adjust color bar outline thickness\n",
    "cbar2d.outline.set_edgecolor('black')  # Set color bar outline color\n",
    "# Save and show the figure\n",
    "output_plot_full_path = os.path.join(output_directory, 'Conformations Landscape - RMSD Colouring.eps')\n",
    "\n",
    "# Save the figure in EPS format\n",
    "try:\n",
    "    plt.savefig(output_plot_full_path, format='eps', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Scatter plot saved as EPS to {output_plot_full_path}\")\n",
    "except PermissionError as e:\n",
    "    print(f\"PermissionError: {e}\")\n",
    "    print(\"Ensure the output path is correct and writable.\")\n",
    "\n",
    "# Also save the figure in TIFF format\n",
    "tiff_output_path = output_plot_full_path.replace('.eps', '.tif')\n",
    "try:\n",
    "    plt.savefig(tiff_output_path, format='tiff', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Scatter plot saved as TIFF to {tiff_output_path}\")\n",
    "except PermissionError as e:\n",
    "    print(f\"PermissionError: {e}\")\n",
    "    print(\"Ensure the output path is correct and writable.\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Optimizing Clustering Parameters with differenet Algorithms and Evaluating Cluster Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import clustering algorithms and evaluation metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import hdbscan\n",
    "\n",
    "# Set the file path\n",
    "file_path = output_filtered_csv_path\n",
    "\n",
    "# Function to determine the best cluster configuration based on metrics\n",
    "def get_best_cluster_config(metric_values, param_values, metric_name, optimize='max'):\n",
    "    if optimize == 'max':\n",
    "        best_index = np.nanargmax(metric_values)\n",
    "    elif optimize == 'min':\n",
    "        best_index = np.nanargmin(metric_values)\n",
    "    else:\n",
    "        raise ValueError(\"Optimize must be 'max' or 'min'\")\n",
    "    return param_values[best_index], metric_values[best_index]\n",
    "\n",
    "# Function to get the best configuration and number of clusters from grid values\n",
    "def get_best_from_grid_with_clusters(grid_values, clusters_grid, param_x, param_y, optimize='max'):\n",
    "    if optimize == 'max':\n",
    "        best_index = np.nanargmax(grid_values)\n",
    "    elif optimize == 'min':\n",
    "        best_index = np.nanargmin(grid_values)\n",
    "    else:\n",
    "        raise ValueError(\"Optimize must be 'max' or 'min'\")\n",
    "    best_coords = np.unravel_index(best_index, grid_values.shape)\n",
    "    return param_x[best_coords[0]], param_y[best_coords[1]], grid_values[best_coords], clusters_grid[best_coords]\n",
    "\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract relevant columns\n",
    "mean_delta_theta = data['Mean_Delta_Theta']\n",
    "mean_delta_phi = data['Mean_Delta_Phi']\n",
    "mean_helix_polarity = data['Mean_Helix_Polarity']\n",
    "mean_rmsd = np.log1p(data['Mean_RMSD'])\n",
    "\n",
    "# Combine features into a single DataFrame\n",
    "X = pd.DataFrame({\n",
    "    'Mean_Delta_Theta': mean_delta_theta,\n",
    "    'Mean_Delta_Phi': mean_delta_phi,\n",
    "    'Mean_Helix_Polarity': mean_helix_polarity,\n",
    "    'Mean_RMSD': mean_rmsd\n",
    "})\n",
    "\n",
    "# Handle missing values (if any)\n",
    "X = X.dropna()\n",
    "\n",
    "feature_weights = {\n",
    "    'Mean_Delta_Theta': 1,   # Adjust weights based on importance\n",
    "    'Mean_Delta_Phi': 1,\n",
    "    'Mean_Helix_Polarity': 1,\n",
    "    'Mean_RMSD': 1\n",
    "}\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert X_scaled back to DataFrame for easy manipulation\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Apply feature weights\n",
    "for feature in X_scaled_df.columns:\n",
    "    weight = feature_weights.get(feature)\n",
    "    X_scaled_df[feature] *= weight\n",
    "\n",
    "# Convert back to NumPy array for clustering algorithms\n",
    "X_weighted = X_scaled_df.values\n",
    "\n",
    "# Function to evaluate clustering\n",
    "def evaluate_clustering(X, labels):\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    print(f'Number of clusters: {n_clusters}')\n",
    "    if n_clusters > 1 and n_clusters < len(labels):\n",
    "        silhouette_avg = silhouette_score(X, labels)\n",
    "        davies_bouldin = davies_bouldin_score(X, labels)\n",
    "        calinski_harabasz = calinski_harabasz_score(X, labels)\n",
    "        print(f'Silhouette Score: {silhouette_avg:.4f}')\n",
    "        print(f'Davies-Bouldin Index: {davies_bouldin:.4f}')\n",
    "        print(f'Calinski-Harabasz Index: {calinski_harabasz:.4f}')\n",
    "    else:\n",
    "        print('Clustering did not produce meaningful clusters.')\n",
    "\n",
    "# KMeans clustering\n",
    "# Initialize lists to store evaluation metrics\n",
    "kmeans_n_clusters = range(2, 10)\n",
    "kmeans_silhouette = []\n",
    "kmeans_davies_bouldin = []\n",
    "kmeans_calinski_harabasz = []\n",
    "\n",
    "for n_clusters in kmeans_n_clusters:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_weighted)\n",
    "    silhouette_avg = silhouette_score(X_weighted, labels)\n",
    "    davies_bouldin = davies_bouldin_score(X_weighted, labels)\n",
    "    calinski_harabasz = calinski_harabasz_score(X_weighted, labels)\n",
    "    kmeans_silhouette.append(silhouette_avg)\n",
    "    kmeans_davies_bouldin.append(davies_bouldin)\n",
    "    kmeans_calinski_harabasz.append(calinski_harabasz)\n",
    "\n",
    "# Plotting the metrics\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(kmeans_n_clusters, kmeans_silhouette, marker='o')\n",
    "plt.title('KMeans Silhouette Score')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(kmeans_n_clusters, kmeans_davies_bouldin, marker='o')\n",
    "plt.title('KMeans Davies-Bouldin Index')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Davies-Bouldin Index')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(kmeans_n_clusters, kmeans_calinski_harabasz, marker='o')\n",
    "plt.title('KMeans Calinski-Harabasz Index')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Calinski-Harabasz Index')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Agglomerative Clustering\n",
    "# Initialize lists\n",
    "agg_n_clusters = range(2, 10)\n",
    "agg_silhouette = []\n",
    "agg_davies_bouldin = []\n",
    "agg_calinski_harabasz = []\n",
    "\n",
    "for n_clusters in agg_n_clusters:\n",
    "    agg = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    labels = agg.fit_predict(X_weighted)\n",
    "    silhouette_avg = silhouette_score(X_weighted, labels)\n",
    "    davies_bouldin = davies_bouldin_score(X_weighted, labels)\n",
    "    calinski_harabasz = calinski_harabasz_score(X_weighted, labels)\n",
    "    agg_silhouette.append(silhouette_avg)\n",
    "    agg_davies_bouldin.append(davies_bouldin)\n",
    "    agg_calinski_harabasz.append(calinski_harabasz)\n",
    "\n",
    "# Plotting the metrics\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(agg_n_clusters, agg_silhouette, marker='o')\n",
    "plt.title('Agglomerative Silhouette Score')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(agg_n_clusters, agg_davies_bouldin, marker='o')\n",
    "plt.title('Agglomerative Davies-Bouldin Index')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Davies-Bouldin Index')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(agg_n_clusters, agg_calinski_harabasz, marker='o')\n",
    "plt.title('Agglomerative Calinski-Harabasz Index')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Calinski-Harabasz Index')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# HDBSCAN\n",
    "import hdbscan\n",
    "\n",
    "# Define a range of min_cluster_size and min_samples values to test\n",
    "min_cluster_size_values = range(2, 11)\n",
    "min_samples_values = range(1, 11)\n",
    "\n",
    "# Prepare a grid to store the metrics\n",
    "hdbscan_silhouette_scores = np.zeros((len(min_cluster_size_values), len(min_samples_values)))\n",
    "hdbscan_davies_bouldin_scores = np.zeros((len(min_cluster_size_values), len(min_samples_values)))\n",
    "hdbscan_calinski_harabasz_scores = np.zeros((len(min_cluster_size_values), len(min_samples_values)))\n",
    "hdbscan_n_clusters_grid = np.zeros((len(min_cluster_size_values), len(min_samples_values)))\n",
    "\n",
    "for i, min_cluster_size in enumerate(min_cluster_size_values):\n",
    "    for j, min_samples in enumerate(min_samples_values):\n",
    "        clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
    "        labels = clusterer.fit_predict(X_weighted)\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        hdbscan_n_clusters_grid[i, j] = n_clusters\n",
    "        if n_clusters > 1 and n_clusters < len(labels):\n",
    "            silhouette_avg = silhouette_score(X_weighted, labels)\n",
    "            davies_bouldin = davies_bouldin_score(X_weighted, labels)\n",
    "            calinski_harabasz = calinski_harabasz_score(X_weighted, labels)\n",
    "            hdbscan_silhouette_scores[i, j] = silhouette_avg\n",
    "            hdbscan_davies_bouldin_scores[i, j] = davies_bouldin\n",
    "            hdbscan_calinski_harabasz_scores[i, j] = calinski_harabasz\n",
    "        else:\n",
    "            hdbscan_silhouette_scores[i, j] = np.nan\n",
    "            hdbscan_davies_bouldin_scores[i, j] = np.nan\n",
    "            hdbscan_calinski_harabasz_scores[i, j] = np.nan\n",
    "\n",
    "# Plot heatmaps\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "sns.heatmap(hdbscan_silhouette_scores, xticklabels=min_samples_values, yticklabels=min_cluster_size_values, ax=axes[0], cmap='viridis', annot=hdbscan_n_clusters_grid.astype(int), fmt='d')\n",
    "axes[0].set_title('HDBSCAN Silhouette Score')\n",
    "axes[0].set_xlabel('min_samples')\n",
    "axes[0].set_ylabel('min_cluster_size')\n",
    "\n",
    "sns.heatmap(hdbscan_davies_bouldin_scores, xticklabels=min_samples_values, yticklabels=min_cluster_size_values, ax=axes[1], cmap='viridis', annot=hdbscan_n_clusters_grid.astype(int), fmt='d')\n",
    "axes[1].set_title('HDBSCAN Davies-Bouldin Index')\n",
    "axes[1].set_xlabel('min_samples')\n",
    "axes[1].set_ylabel('min_cluster_size')\n",
    "\n",
    "sns.heatmap(hdbscan_calinski_harabasz_scores, xticklabels=min_samples_values, yticklabels=min_cluster_size_values, ax=axes[2], cmap='viridis', annot=hdbscan_n_clusters_grid.astype(int), fmt='d')\n",
    "axes[2].set_title('HDBSCAN Calinski-Harabasz Index')\n",
    "axes[2].set_xlabel('min_samples')\n",
    "axes[2].set_ylabel('min_cluster_size')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Best configurations for KMeans\n",
    "best_kmeans_silhouette = get_best_cluster_config(kmeans_silhouette, kmeans_n_clusters, 'Silhouette', optimize='max')\n",
    "best_kmeans_davies_bouldin = get_best_cluster_config(kmeans_davies_bouldin, kmeans_n_clusters, 'Davies-Bouldin', optimize='min')\n",
    "best_kmeans_calinski_harabasz = get_best_cluster_config(kmeans_calinski_harabasz, kmeans_n_clusters, 'Calinski-Harabasz', optimize='max')\n",
    "\n",
    "# Best configurations for Agglomerative Clustering\n",
    "best_agg_silhouette = get_best_cluster_config(agg_silhouette, agg_n_clusters, 'Silhouette', optimize='max')\n",
    "best_agg_davies_bouldin = get_best_cluster_config(agg_davies_bouldin, agg_n_clusters, 'Davies-Bouldin', optimize='min')\n",
    "best_agg_calinski_harabasz = get_best_cluster_config(agg_calinski_harabasz, agg_n_clusters, 'Calinski-Harabasz', optimize='max')\n",
    "\n",
    "\n",
    "\n",
    "# Best configurations for HDBSCAN\n",
    "best_hdbscan_silhouette = get_best_from_grid_with_clusters(\n",
    "    hdbscan_silhouette_scores, hdbscan_n_clusters_grid, min_cluster_size_values, min_samples_values, optimize='max'\n",
    ")\n",
    "best_hdbscan_davies_bouldin = get_best_from_grid_with_clusters(\n",
    "    hdbscan_davies_bouldin_scores, hdbscan_n_clusters_grid, min_cluster_size_values, min_samples_values, optimize='min'\n",
    ")\n",
    "best_hdbscan_calinski_harabasz = get_best_from_grid_with_clusters(\n",
    "    hdbscan_calinski_harabasz_scores, hdbscan_n_clusters_grid, min_cluster_size_values, min_samples_values, optimize='max'\n",
    ")\n",
    "\n",
    "# Print best configurations including the number of clusters\n",
    "print(\"Best Cluster Configurations:\")\n",
    "print(\"\\nKMeans:\")\n",
    "print(f\"  Best Silhouette Score: {best_kmeans_silhouette}\")\n",
    "print(f\"  Best Davies-Bouldin Index: {best_kmeans_davies_bouldin}\")\n",
    "print(f\"  Best Calinski-Harabasz Index: {best_kmeans_calinski_harabasz}\")\n",
    "\n",
    "print(\"\\nAgglomerative Clustering:\")\n",
    "print(f\"  Best Silhouette Score: {best_agg_silhouette}\")\n",
    "print(f\"  Best Davies-Bouldin Index: {best_agg_davies_bouldin}\")\n",
    "print(f\"  Best Calinski-Harabasz Index: {best_agg_calinski_harabasz}\")\n",
    "\n",
    "print(\"\\nHDBSCAN:\")\n",
    "print(f\"  Best Silhouette Score: min_cluster_size={best_hdbscan_silhouette[0]}, min_samples={best_hdbscan_silhouette[1]}, \"\n",
    "      f\"score={best_hdbscan_silhouette[2]:.4f}, clusters={int(best_hdbscan_silhouette[3])}\")\n",
    "print(f\"  Best Davies-Bouldin Index: min_cluster_size={best_hdbscan_davies_bouldin[0]}, min_samples={best_hdbscan_davies_bouldin[1]}, \"\n",
    "      f\"score={best_hdbscan_davies_bouldin[2]:.4f}, clusters={int(best_hdbscan_davies_bouldin[3])}\")\n",
    "print(f\"  Best Calinski-Harabasz Index: min_cluster_size={best_hdbscan_calinski_harabasz[0]}, min_samples={best_hdbscan_calinski_harabasz[1]}, \"\n",
    "      f\"score={best_hdbscan_calinski_harabasz[2]:.4f}, clusters={int(best_hdbscan_calinski_harabasz[3])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7A. Clustering with KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Import clustering algorithms\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# Reset all matplotlib settings to their default values\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "\n",
    "#Define the numbers of clusters after running the different scores above \n",
    "clusters = 2\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract relevant columns\n",
    "mean_delta_theta = data['Mean_Delta_Theta']\n",
    "mean_delta_phi = data['Mean_Delta_Phi']\n",
    "mean_helix_polarity = data['Mean_Helix_Polarity']\n",
    "mean_rmsd = np.log1p(data['Mean_RMSD'])\n",
    "\n",
    "# Combine features into a single DataFrame\n",
    "X = pd.DataFrame({\n",
    "    'Mean_Delta_Theta': mean_delta_theta,\n",
    "    'Mean_Delta_Phi': mean_delta_phi,\n",
    "    'Mean_Helix_Polarity': mean_helix_polarity,\n",
    "    'Mean_RMSD': mean_rmsd\n",
    "})\n",
    "\n",
    "# Handle missing values (if any)\n",
    "X = X.dropna()\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert X_scaled back to DataFrame\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Define feature weights (replace these with your actual weights)\n",
    "feature_weights = {\n",
    "    'Mean_Delta_Theta': 1,    # Adjust weights based on your PCA results\n",
    "    'Mean_Delta_Phi': 1,\n",
    "    'Mean_Helix_Polarity': 1,\n",
    "    'Mean_RMSD': 1\n",
    "}\n",
    "\n",
    "# Apply feature weights\n",
    "for feature in X_scaled_df.columns:\n",
    "    weight = feature_weights.get(feature)\n",
    "    X_scaled_df[feature] *= weight\n",
    "\n",
    "# Convert back to NumPy array for clustering algorithms\n",
    "X_weighted = X_scaled_df.values\n",
    "\n",
    "# Perform K-means Clustering with 4 clusters\n",
    "kmeans = KMeans(n_clusters=clusters, random_state=42)\n",
    "labels = kmeans.fit_predict(X_weighted)\n",
    "\n",
    "# Add cluster labels to data\n",
    "data['Cluster'] = labels\n",
    "\n",
    "# Plotting parameters\n",
    "colors = sns.color_palette('Set1', n_colors=clusters)  # Colors for 4 clusters\n",
    "\n",
    "# Create a figure with a 2D plot and a 3D plot\n",
    "fig = plt.figure(figsize=(18, 7), constrained_layout=True)\n",
    "\n",
    "# Add a central title\n",
    "fig.suptitle('K-Means Clustering', fontsize=16, fontweight='bold')\n",
    "\n",
    "\n",
    "# 2D Plot: K-means Clustering\n",
    "ax2d = fig.add_subplot(1, 2, 2)\n",
    "for cluster in range(len(np.unique(labels))):\n",
    "    cluster_data = data[data['Cluster'] == cluster]\n",
    "    ax2d.scatter(cluster_data['Mean_Delta_Theta'], cluster_data['Mean_Delta_Phi'],\n",
    "                color=colors[cluster], edgecolor='black', s=80, label=f'Cluster {cluster}')\n",
    "ax2d.set_xlabel(r'$\\boldsymbol{\\Delta\\theta} \\, \\boldsymbol{(°)}$', fontsize=13, fontweight='bold')\n",
    "ax2d.set_ylabel(r'$\\boldsymbol{\\Delta\\phi} \\, \\boldsymbol{(°)}$', fontsize=13, fontweight='bold')\n",
    "ax2d.set_title('2D Scatter Plot', fontsize=14, fontweight='bold')\n",
    "ax2d.grid(color='gray', linestyle='--', linewidth=0.3)\n",
    "\n",
    "ax2d.axhline(0, color='black', linestyle=(0, (5, 5)), linewidth=1.2)  # Dashed Phi=0 line\n",
    "ax2d.axvline(0, color='black', linestyle=(0, (5, 5)), linewidth=1.2)  # Dashed Theta=0 line\n",
    "\n",
    "# Adjust tick size and font\n",
    "ax2d.tick_params(axis='both', which='major', length=8, width=1.5, labelsize=12)  # Major ticks\n",
    "ax2d.tick_params(axis='both', which='minor', length=4, width=1.0, labelsize=10)  # Minor ticks (optional)\n",
    "\n",
    "# Adjust graph (line) thickness\n",
    "# Adjust graph line thickness and set spine colors to black\n",
    "ax2d.spines['top'].set_linewidth(1.5)\n",
    "ax2d.spines['top'].set_color('black')\n",
    "ax2d.spines['right'].set_linewidth(1.5)\n",
    "ax2d.spines['right'].set_color('black')\n",
    "ax2d.spines['bottom'].set_linewidth(1.5)\n",
    "ax2d.spines['bottom'].set_color('black')\n",
    "ax2d.spines['left'].set_linewidth(1.5)\n",
    "ax2d.spines['left'].set_color('black')\n",
    "\n",
    "ax2d.legend()\n",
    "\n",
    "\n",
    "# 3D Scatter Plot\n",
    "ax3d = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "\n",
    "# Plot clusters in 3D\n",
    "for cluster in range(len(np.unique(labels))):\n",
    "    cluster_data = data[data['Cluster'] == cluster]\n",
    "    ax3d.scatter(cluster_data['Mean_Delta_Theta'], \n",
    "                 cluster_data['Mean_Delta_Phi'], \n",
    "                 cluster_data['Mean_Helix_Polarity'],\n",
    "                 color=colors[cluster], edgecolor='black', s=60, label=f'Cluster {cluster}')\n",
    "\n",
    "# Set labels for the axes and the title\n",
    "ax3d.set_xlabel(r'$\\boldsymbol{\\Delta\\theta} \\, \\boldsymbol{(°)}$', fontsize=15, fontweight='bold')\n",
    "ax3d.set_ylabel(r'$\\boldsymbol{\\Delta\\phi} \\, \\boldsymbol{(°)}$', fontsize=15, fontweight='bold')\n",
    "ax3d.set_zlabel('Helix Polarity', fontsize=15, fontweight='bold')\n",
    "ax3d.set_title('3D Conformational Space (ΔΘ, ΔΦ, Polarity)', fontsize=16, fontweight='bold', x=0.6)\n",
    "\n",
    "# Adjust the thickness of the gridlines and axis lines\n",
    "ax3d.tick_params(axis='x', colors='black', width=1.5)\n",
    "ax3d.tick_params(axis='y', colors='black', width=1.5)\n",
    "ax3d.tick_params(axis='z', colors='black', width=1.5)\n",
    "\n",
    "ax3d.grid(True, linewidth=1.5)\n",
    "ax3d.grid(color='gray', linestyle='--', linewidth=0.3)\n",
    "\n",
    "# Make axis lines black and thicker\n",
    "ax3d.xaxis._axinfo[\"axisline\"][\"color\"] = (0,0,0,1)  # Black\n",
    "ax3d.xaxis._axinfo[\"axisline\"][\"linewidth\"] = 2.0\n",
    "\n",
    "ax3d.yaxis._axinfo[\"axisline\"][\"color\"] = (0,0,0,1)  # Black\n",
    "ax3d.yaxis._axinfo[\"axisline\"][\"linewidth\"] = 2.0\n",
    "\n",
    "ax3d.zaxis._axinfo[\"axisline\"][\"color\"] = (0,0,0,1)  # Black\n",
    "ax3d.zaxis._axinfo[\"axisline\"][\"linewidth\"] = 2.0\n",
    "\n",
    "# Adjust the ticks for the axes\n",
    "ax3d.set_zticks([-1, 0, 1])  # Helix polarity axis: only +1 and -1\n",
    "ax3d.set_yticks(np.arange(-200, 201, 100))  # Delta Phi axis: ticks in -100 steps\n",
    "\n",
    "# Adjust tick size and font\n",
    "ax3d.tick_params(axis='both', which='major', length=8, width=1.5, labelsize=12)\n",
    "ax3d.tick_params(axis='both', which='minor', length=4, width=1.0, labelsize=10)\n",
    "\n",
    "# Reference lines\n",
    "ax3d.plot([0, 0], [-200, 200], [1.0, 1.0], color='black', linestyle=(0, (5, 5)), linewidth=1.2)\n",
    "ax3d.plot([-80, 40], [0, 0], [1.0, 1.0], color='black', linestyle=(0, (5, 5)), linewidth=1.2)\n",
    "ax3d.plot([0, 0], [0, 0], [-1, 1.2], color='black', linestyle=(0, (5, 5)), linewidth=1.2)\n",
    "\n",
    "#Add legend\n",
    "ax3d.legend(loc='lower right', fontsize=10, bbox_to_anchor=(1.15, 0))\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "output_file_name = 'all_combined_results_with_Kmeans_clusters.csv'  # Desired output file name\n",
    "output_filtered_csv_path = os.path.join(output_directory, output_file_name)  # Combine directory and file name\n",
    "data.to_csv(output_filtered_csv_path, index=False)\n",
    "\n",
    "# Save the figure (optional)\n",
    "fig.savefig(os.path.join(output_directory, \"K-Means_Cluster.eps\"), format='eps', dpi=300, bbox_inches=\"tight\")\n",
    "fig.savefig(os.path.join(output_directory, \"K-Means_Cluster.tif\"), format='tif', dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Read the clustered data\n",
    "data = pd.read_csv(output_filtered_csv_path)  # Replace with your actual file path\n",
    "\n",
    "# Relevant columns\n",
    "metrics = [\n",
    "    \"PSSM_Score\",\n",
    "    \"Mean_ipTM\",\n",
    "    \"Mean_RMSD\",\n",
    "#    \"Helix_Score\",\n",
    "#    \"Coil_Score\",\n",
    "#    \"Strand_Score\",\n",
    "    \"IUPRED_Score\",\n",
    "    \"ANCHOR_Score\",\n",
    "#    \"Mean_Delta_Theta\",\n",
    "#    \"Mean_Delta_Phi\",\n",
    "#    \"Mean_Helix_Polarity\",\n",
    "]\n",
    "\n",
    "# Ensure 'Cluster' column exists\n",
    "if \"Cluster\" not in data.columns:\n",
    "    raise ValueError(\"The 'Cluster' column is not in the dataset. Please check the input file.\")\n",
    "\n",
    "# Group by cluster and calculate mean and SEM\n",
    "cluster_stats = data.groupby(\"Cluster\")[metrics].agg([\"mean\", \"sem\"]).reset_index()\n",
    "\n",
    "# Flatten MultiIndex columns for easier referencing\n",
    "cluster_stats.columns = [f\"{col[0]}_{col[1]}\" if col[1] else col[0] for col in cluster_stats.columns]\n",
    "\n",
    "# Custom y-axis limits for certain metrics\n",
    "custom_y_limits = {\n",
    "    \"Mean_ipTM\": (0, 1),           # Example: [Min, Max]\n",
    "    \"Helix_Score\": (0, 1),\n",
    "    \"Coil_Score\": (0, 1),\n",
    "    \"Strand_Score\": (0, 1),\n",
    "    \"IUPRED_Score\": (0, 1),\n",
    "    \"ANCHOR_Score\": (0, 1),\n",
    "    \"Mean_Delta_Theta\": (-180, 180),\n",
    "    \"Mean_Delta_Phi\": (-180, 180),\n",
    "    \"Mean_Helix_Polarity\": (-1.1, 1.1),\n",
    "}\n",
    "\n",
    "# Custom titles for metrics with Greek letters and descriptive names\n",
    "metric_titles = {\n",
    "    \"PSSM_Score\": r\"PSSM Score\",\n",
    "    \"Mean_ipTM\": r\"ipTM Score\",\n",
    "    \"Mean_RMSD\": r\"RMSD (Å)\",  # Example: Adding Å for RMSD in angstroms\n",
    "    \"Helix_Score\": r\"Helix Score\",\n",
    "    \"Coil_Score\": r\"Coil Score\",\n",
    "    \"Strand_Score\": r\"Strand Score\",\n",
    "    \"IUPRED_Score\": r\"IUPRED Score\",\n",
    "    \"ANCHOR_Score\": r\"ANCHOR Score\",\n",
    "    \"Mean_Delta_Theta\": r\"$\\boldsymbol{\\Delta\\theta} \\, \\boldsymbol{(°)}$\",  # Greek delta for Theta\n",
    "    \"Mean_Delta_Phi\": r\"$\\boldsymbol{\\Delta\\phi} \\, \\boldsymbol{(°)}$\",      # Greek delta for Phi\n",
    "    \"Mean_Helix_Polarity\": r\"Helix Polarity\",\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "sns.set(style=\"whitegrid\")  # Use a clean white grid for the background\n",
    "fig, axes = plt.subplots(1, len(metrics), figsize=(25, 5))\n",
    "\n",
    "# Adjust subplot spacing\n",
    "fig.subplots_adjust(left=0.05, right=0.95, top=0.9, bottom=0.15, wspace=0.4)  # Increase wspace for horizontal spacing\n",
    "\n",
    "# Define a pastel color palette\n",
    "palette = sns.color_palette(\"pastel\", n_colors=len(metrics))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    means = cluster_stats[f\"{metric}_mean\"]\n",
    "    sems = cluster_stats[f\"{metric}_sem\"]\n",
    "    clusters = cluster_stats[\"Cluster\"]\n",
    "    \n",
    "    # Bar plot with error bars\n",
    "    bars = ax.bar(clusters, means, yerr=sems, capsize=3, color=palette[i], edgecolor=\"black\")\n",
    "    \n",
    "    # Set title and labels\n",
    "    title = metric_titles.get(metric, metric).replace(\"_\", \" \")\n",
    "    ax.set_title(title, fontsize=21, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Cluster\", fontsize=20, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Mean Value\", fontsize=20, fontweight=\"bold\")\n",
    "    \n",
    "    # Apply custom y-axis limits if specified\n",
    "    if metric in custom_y_limits:\n",
    "        ymin, ymax = custom_y_limits[metric]\n",
    "        ax.set_ylim(ymin, ymax)\n",
    "    \n",
    "    # Explicitly set x-axis ticks and labels\n",
    "    ax.set_xticks(clusters)  # Set ticks at the cluster positions\n",
    "    ax.set_xticklabels(clusters.astype(int), fontsize=18)  # Ensure labels match the cluster numbers\n",
    "    \n",
    "    # Explicitly set y-axis ticks\n",
    "    yticks = np.linspace(ax.get_ylim()[0], ax.get_ylim()[1], 5)  # Generate 5 evenly spaced ticks\n",
    "    ax.set_yticks(yticks)  # Set y-ticks\n",
    "    ax.set_yticklabels([f\"{tick:.2f}\" for tick in yticks], fontsize=18)  # Format y-tick labels\n",
    "    \n",
    "    # Enable grid only for the y-axis (horizontal lines)\n",
    "    ax.grid(axis=\"y\", color=\"gray\", linestyle=\"--\", linewidth=0.5)  # Horizontal lines only\n",
    "    ax.grid(axis=\"x\", visible=False)  # Disable x-axis grid lines\n",
    "\n",
    "\n",
    "    # Adjust the line thickness and colors\n",
    "    ax.spines['bottom'].set_linewidth(1.5)  \n",
    "    ax.spines['left'].set_linewidth(1.5)    \n",
    "    ax.spines['right'].set_linewidth(1.5)   \n",
    "    ax.spines['top'].set_linewidth(1.5)     \n",
    "\n",
    "    ax.spines['bottom'].set_color('black')  \n",
    "    ax.spines['left'].set_color('black')    \n",
    "    ax.spines['right'].set_color('black')   \n",
    "    ax.spines['top'].set_color('black')     \n",
    "\n",
    "# Add a central title\n",
    "fig.suptitle(\"Cluster-wise Metrics of K-Means\", fontsize=18, fontweight=\"bold\", y=1.02)\n",
    "\n",
    "# Add a central title\n",
    "fig.suptitle(\"Cluster-wise Metrics of K-Means\", fontsize=16, fontweight=\"bold\", y=1.05)\n",
    "\n",
    "# Save the figure (optional)\n",
    "fig.savefig(os.path.join(output_directory, \"K-Means_Cluster_Metadata.eps\"), format='eps', dpi=300, bbox_inches=\"tight\")\n",
    "fig.savefig(os.path.join(output_directory, \"K-Means_Cluster_Metadata.tif\"), format='tif', dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# shift y axis tic labels with ha='right' to align \n",
    "# correct the format of the figure when creating it (f.e. always 18 to 7) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7B. Clustering with Agglomerative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import clustering algorithms and evaluation metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# For plotting ellipses\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "import matplotlib as mpl\n",
    "# Reset all matplotlib settings to their default values\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "clusters=2\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract relevant columns\n",
    "mean_delta_theta = data['Mean_Delta_Theta']\n",
    "mean_delta_phi = data['Mean_Delta_Phi']\n",
    "mean_helix_polarity = data['Mean_Helix_Polarity']\n",
    "mean_rmsd = np.log1p(data['Mean_RMSD'])\n",
    "\n",
    "# Combine features into a single DataFrame\n",
    "X = pd.DataFrame({\n",
    "    'Mean_Delta_Theta': mean_delta_theta,\n",
    "    'Mean_Delta_Phi': mean_delta_phi,\n",
    "    'Mean_Helix_Polarity': mean_helix_polarity,\n",
    "    'Mean_RMSD': mean_rmsd\n",
    "})\n",
    "\n",
    "# Handle missing values (if any)\n",
    "X = X.dropna()\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert X_scaled back to DataFrame\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Define feature weights (replace these with your actual weights)\n",
    "feature_weights = {\n",
    "    'Mean_Delta_Theta': 1,    # Adjust weights based on your PCA results\n",
    "    'Mean_Delta_Phi': 1,\n",
    "    'Mean_Helix_Polarity': 1,\n",
    "    'Mean_RMSD': 1\n",
    "}\n",
    "\n",
    "# Apply feature weights\n",
    "for feature in X_scaled_df.columns:\n",
    "    weight = feature_weights.get(feature)\n",
    "    X_scaled_df[feature] *= weight\n",
    "\n",
    "# Convert back to NumPy array for clustering algorithms\n",
    "X_weighted = X_scaled_df.values\n",
    "\n",
    "# Perform Agglomerative Clustering with 4 clusters\n",
    "agg = AgglomerativeClustering(n_clusters=clusters)\n",
    "labels = agg.fit_predict(X_weighted)\n",
    "\n",
    "# Add cluster labels to data\n",
    "data['Cluster'] = labels\n",
    "\n",
    "# For plotting, extract relevant columns\n",
    "plot_data = pd.DataFrame({\n",
    "    'Mean_Delta_Phi': mean_delta_phi,\n",
    "    'Mean_Delta_Theta': mean_delta_theta,\n",
    "    'Cluster': labels,\n",
    "    'Mean_Helix_Polarity': mean_helix_polarity,\n",
    "    'Mean_RMSD': mean_rmsd # This should be the transformed Mean_RMSD\n",
    "})\n",
    "\n",
    "# Define colors for clusters\n",
    "num_clusters = len(np.unique(labels))\n",
    "colors = sns.color_palette('Set1', n_colors=num_clusters)\n",
    "\n",
    "# Create a figure with a 2D plot and a 3D plot\n",
    "fig = plt.figure(figsize=(18, 7), constrained_layout=True)\n",
    "\n",
    "# Add a central title\n",
    "fig.suptitle('Agglomerative Clustering', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 3D Scatter Plot\n",
    "ax3d = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "\n",
    "# 3D scatter plot with cluster colors\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_data = plot_data[plot_data['Cluster'] == cluster]\n",
    "    ax3d.scatter(cluster_data['Mean_Delta_Theta'], \n",
    "                 cluster_data['Mean_Delta_Phi'], \n",
    "                 cluster_data['Mean_Helix_Polarity'],\n",
    "                 color=colors[cluster], edgecolor='k', s=50, label=f'Cluster {cluster}')\n",
    "\n",
    "# Set labels for the axes and the title\n",
    "ax3d.set_xlabel(r'$\\boldsymbol{\\Delta\\theta} \\, \\boldsymbol{(°)}$', fontsize=15, fontweight='bold')\n",
    "ax3d.set_ylabel(r'$\\boldsymbol{\\Delta\\phi} \\, \\boldsymbol{(°)}$', fontsize=15, fontweight='bold')\n",
    "ax3d.set_zlabel('Helix Polarity', fontsize=15, fontweight='bold')\n",
    "ax3d.set_title('3D Conformational Space (ΔΘ, ΔΦ, Polarity)', fontsize=16, fontweight='bold', x=0.6)\n",
    "\n",
    "# Adjust the thickness of the gridlines and axis lines\n",
    "ax3d.tick_params(axis='x', colors='black', width=1.5)\n",
    "ax3d.tick_params(axis='y', colors='black', width=1.5)\n",
    "ax3d.tick_params(axis='z', colors='black', width=1.5)\n",
    "\n",
    "ax3d.grid(True, linewidth=1.5)\n",
    "ax3d.grid(color='gray', linestyle='--', linewidth=0.3)\n",
    "\n",
    "# Make axis lines black and thicker\n",
    "ax3d.xaxis._axinfo[\"axisline\"][\"color\"] = (0,0,0,1)  # Black\n",
    "ax3d.xaxis._axinfo[\"axisline\"][\"linewidth\"] = 2.0\n",
    "\n",
    "ax3d.yaxis._axinfo[\"axisline\"][\"color\"] = (0,0,0,1)  # Black\n",
    "ax3d.yaxis._axinfo[\"axisline\"][\"linewidth\"] = 2.0\n",
    "\n",
    "ax3d.zaxis._axinfo[\"axisline\"][\"color\"] = (0,0,0,1)  # Black\n",
    "ax3d.zaxis._axinfo[\"axisline\"][\"linewidth\"] = 2.0\n",
    "\n",
    "# Adjust the ticks for the axes\n",
    "ax3d.set_zticks([-1, 0, 1])  # Helix polarity axis: only +1 and -1\n",
    "ax3d.set_yticks(np.arange(-200, 201, 100))  # Delta Phi axis: ticks in -100 steps\n",
    "\n",
    "# Adjust tick size and font\n",
    "ax3d.tick_params(axis='both', which='major', length=8, width=1.5, labelsize=12)\n",
    "ax3d.tick_params(axis='both', which='minor', length=4, width=1.0, labelsize=10)\n",
    "\n",
    "# Reference lines\n",
    "ax3d.plot([0, 0], [-200, 200], [1.0, 1.0], color='black', linestyle=(0, (5, 5)), linewidth=1.2)\n",
    "ax3d.plot([-80, 40], [0, 0], [1.0, 1.0], color='black', linestyle=(0, (5, 5)), linewidth=1.2)\n",
    "ax3d.plot([0, 0], [0, 0], [-1, 1.2], color='black', linestyle=(0, (5, 5)), linewidth=1.2)\n",
    "\n",
    "#Add legend\n",
    "ax3d.legend(loc='lower right', fontsize=10, bbox_to_anchor=(1.15, 0))\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "output_file_name = 'all_combined_results_with_Kmeans_clusters.csv'  # Desired output file name\n",
    "output_filtered_csv_path = os.path.join(output_directory, output_file_name)  # Combine directory and file name\n",
    "data.to_csv(output_filtered_csv_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "# 2D Plot: K-means Clustering\n",
    "ax2d = fig.add_subplot(1, 2, 2)\n",
    "for cluster in range(len(np.unique(labels))):\n",
    "    cluster_data = data[data['Cluster'] == cluster]\n",
    "    ax2d.scatter(cluster_data['Mean_Delta_Theta'], cluster_data['Mean_Delta_Phi'],\n",
    "                color=colors[cluster], edgecolor='k', s=50, label=f'Cluster {cluster}')\n",
    "ax2d.set_xlabel(r'$\\boldsymbol{\\Delta\\theta} \\, \\boldsymbol{(°)}$', fontsize=13, fontweight='bold')\n",
    "ax2d.set_ylabel(r'$\\boldsymbol{\\Delta\\phi} \\, \\boldsymbol{(°)}$', fontsize=13, fontweight='bold')\n",
    "ax2d.set_title('2D Scatter Plot', fontsize=14, fontweight='bold')\n",
    "ax2d.grid(color='gray', linestyle='--', linewidth=0.3)\n",
    "\n",
    "ax2d.axhline(0, color='black', linestyle=(0, (5, 5)), linewidth=1.2)  # Dashed Phi=0 line\n",
    "ax2d.axvline(0, color='black', linestyle=(0, (5, 5)), linewidth=1.2)  # Dashed Theta=0 line\n",
    "\n",
    "# Adjust tick size and font\n",
    "ax2d.tick_params(axis='both', which='major', length=8, width=1.5, labelsize=12)  # Major ticks\n",
    "ax2d.tick_params(axis='both', which='minor', length=4, width=1.0, labelsize=10)  # Minor ticks (optional)\n",
    "\n",
    "# Adjust graph (line) thickness\n",
    "ax2d.spines['top'].set_linewidth(1.5)  # Top border\n",
    "ax2d.spines['right'].set_linewidth(1.5)  # Right border\n",
    "ax2d.spines['left'].set_linewidth(1.5)  # Left border\n",
    "ax2d.spines['bottom'].set_linewidth(1.5)  # Bottom border\n",
    "\n",
    "ax2d.legend()\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "output_file_name = 'all_combined_results_with_Agglomerative_clusters.csv'  # Desired output file name\n",
    "output_filtered_csv_path = os.path.join(output_directory, output_file_name)  # Combine directory and file name\n",
    "data.to_csv(output_filtered_csv_path, index=False)\n",
    "\n",
    "# Save the figure (optional)\n",
    "fig.savefig(os.path.join(output_directory, \"Agglomerative_Cluster.eps\"), format='eps', dpi=300, bbox_inches=\"tight\")\n",
    "fig.savefig(os.path.join(output_directory, \"Agglomerative_Cluster.tif\"), format='tif', dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "# Read the clustered data\n",
    "data = pd.read_csv(output_filtered_csv_path)  # Replace with your actual file path\n",
    "\n",
    "# Relevant columns\n",
    "metrics = [\n",
    "    \"PSSM_Score\",\n",
    "    \"Mean_ipTM\",\n",
    "    \"Mean_RMSD\",\n",
    "#    \"Helix_Score\",\n",
    "#    \"Coil_Score\",\n",
    "#    \"Strand_Score\",\n",
    "    \"IUPRED_Score\",\n",
    "    \"ANCHOR_Score\",\n",
    "#    \"Mean_Delta_Theta\",\n",
    "#    \"Mean_Delta_Phi\",\n",
    "#    \"Mean_Helix_Polarity\",\n",
    "]\n",
    "\n",
    "# Ensure 'Cluster' column exists\n",
    "if \"Cluster\" not in data.columns:\n",
    "    raise ValueError(\"The 'Cluster' column is not in the dataset. Please check the input file.\")\n",
    "\n",
    "# Group by cluster and calculate mean and SEM\n",
    "cluster_stats = data.groupby(\"Cluster\")[metrics].agg([\"mean\", \"sem\"]).reset_index()\n",
    "\n",
    "# Flatten MultiIndex columns for easier referencing\n",
    "cluster_stats.columns = [f\"{col[0]}_{col[1]}\" if col[1] else col[0] for col in cluster_stats.columns]\n",
    "\n",
    "# Custom y-axis limits for certain metrics\n",
    "custom_y_limits = {\n",
    "    \"Mean_ipTM\": (0, 1),           # Example: [Min, Max]\n",
    "    \"Helix_Score\": (0, 1),\n",
    "    \"Coil_Score\": (0, 1),\n",
    "    \"Strand_Score\": (0, 1),\n",
    "    \"IUPRED_Score\": (0, 1),\n",
    "    \"ANCHOR_Score\": (0, 1),\n",
    "    \"Mean_Delta_Theta\": (-180, 180),\n",
    "    \"Mean_Delta_Phi\": (-180, 180),\n",
    "    \"Mean_Helix_Polarity\": (-1.1, 1.1),\n",
    "}\n",
    "\n",
    "# Custom titles for metrics with Greek letters and descriptive names\n",
    "metric_titles = {\n",
    "    \"PSSM_Score\": r\"PSSM Score\",\n",
    "    \"Mean_ipTM\": r\"ipTM Score\",\n",
    "    \"Mean_RMSD\": r\"RMSD (Å)\",  # Example: Adding Å for RMSD in angstroms\n",
    "    \"Helix_Score\": r\"Helix Score\",\n",
    "    \"Coil_Score\": r\"Coil Score\",\n",
    "    \"Strand_Score\": r\"Strand Score\",\n",
    "    \"IUPRED_Score\": r\"IUPRED Score\",\n",
    "    \"ANCHOR_Score\": r\"ANCHOR Score\",\n",
    "    \"Mean_Delta_Theta\": r\"$\\boldsymbol{\\Delta\\theta} \\, \\boldsymbol{(°)}$\",  # Greek delta for Theta\n",
    "    \"Mean_Delta_Phi\": r\"$\\boldsymbol{\\Delta\\phi} \\, \\boldsymbol{(°)}$\",      # Greek delta for Phi\n",
    "    \"Mean_Helix_Polarity\": r\"Helix Polarity\",\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "sns.set(style=\"whitegrid\")  # Use a clean white grid for the background\n",
    "fig, axes = plt.subplots(1, len(metrics), figsize=(25, 5))\n",
    "\n",
    "# Adjust subplot spacing\n",
    "fig.subplots_adjust(left=0.05, right=0.95, top=0.9, bottom=0.15, wspace=0.4)  # Increase wspace for horizontal spacing\n",
    "\n",
    "# Define a pastel color palette\n",
    "palette = sns.color_palette(\"pastel\", n_colors=len(metrics))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    means = cluster_stats[f\"{metric}_mean\"]\n",
    "    sems = cluster_stats[f\"{metric}_sem\"]\n",
    "    clusters = cluster_stats[\"Cluster\"]\n",
    "    \n",
    "    # Bar plot with error bars\n",
    "    bars = ax.bar(clusters, means, yerr=sems, capsize=3, color=palette[i], edgecolor=\"black\")\n",
    "    \n",
    "    # Set title and labels\n",
    "    title = metric_titles.get(metric, metric).replace(\"_\", \" \")\n",
    "    ax.set_title(title, fontsize=21, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Cluster\", fontsize=20, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Mean Value\", fontsize=20, fontweight=\"bold\")\n",
    "    \n",
    "    # Apply custom y-axis limits if specified\n",
    "    if metric in custom_y_limits:\n",
    "        ymin, ymax = custom_y_limits[metric]\n",
    "        ax.set_ylim(ymin, ymax)\n",
    "    \n",
    "    # Explicitly set x-axis ticks and labels\n",
    "    ax.set_xticks(clusters)  # Set ticks at the cluster positions\n",
    "    ax.set_xticklabels(clusters.astype(int), fontsize=18)  # Ensure labels match the cluster numbers\n",
    "    \n",
    "    # Explicitly set y-axis ticks\n",
    "    yticks = np.linspace(ax.get_ylim()[0], ax.get_ylim()[1], 5)  # Generate 5 evenly spaced ticks\n",
    "    ax.set_yticks(yticks)  # Set y-ticks\n",
    "    ax.set_yticklabels([f\"{tick:.2f}\" for tick in yticks], fontsize=18)  # Format y-tick labels\n",
    "    \n",
    "    # Enable grid only for the y-axis (horizontal lines)\n",
    "    ax.grid(axis=\"y\", color=\"gray\", linestyle=\"--\", linewidth=0.5)  # Horizontal lines only\n",
    "    ax.grid(axis=\"x\", visible=False)  # Disable x-axis grid lines\n",
    "\n",
    "\n",
    "    # Adjust the line thickness and colors\n",
    "    ax.spines['bottom'].set_linewidth(1.5)  \n",
    "    ax.spines['left'].set_linewidth(1.5)    \n",
    "    ax.spines['right'].set_linewidth(1.5)   \n",
    "    ax.spines['top'].set_linewidth(1.5)     \n",
    "\n",
    "    ax.spines['bottom'].set_color('black')  \n",
    "    ax.spines['left'].set_color('black')    \n",
    "    ax.spines['right'].set_color('black')   \n",
    "    ax.spines['top'].set_color('black')     \n",
    "\n",
    "# Add a central title\n",
    "fig.suptitle(\"Cluster-wise Metrics of K-Means\", fontsize=18, fontweight=\"bold\", y=1.02)\n",
    "\n",
    "# Add a central title\n",
    "fig.suptitle(\"Cluster-wise Metrics of K-Means\", fontsize=16, fontweight=\"bold\", y=1.05)\n",
    "\n",
    "# Save the figure (optional)\n",
    "fig.savefig(os.path.join(output_directory, \"Agglomerative_Cluster_Metadata.eps\"), format='eps', dpi=300, bbox_inches=\"tight\")\n",
    "fig.savefig(os.path.join(output_directory, \"Agglomerative_Cluster_Metadata.tif\"), format='tif', dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7C. Clustering with HDBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import clustering algorithms\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import hdbscan\n",
    "import matplotlib as mpl\n",
    "# Reset all matplotlib settings to their default values\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "\n",
    "min_cluster_size=2\n",
    "min_samples=2\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract relevant columns\n",
    "mean_delta_theta = data['Mean_Delta_Theta']\n",
    "mean_delta_phi = data['Mean_Delta_Phi']\n",
    "mean_helix_polarity = data['Mean_Helix_Polarity']\n",
    "mean_rmsd = np.log1p(data['Mean_RMSD'])\n",
    "\n",
    "# Combine features into a single DataFrame\n",
    "X = pd.DataFrame({\n",
    "    'Mean_Delta_Theta': mean_delta_theta,\n",
    "    'Mean_Delta_Phi': mean_delta_phi,\n",
    "    'Mean_Helix_Polarity': mean_helix_polarity,\n",
    "    'Mean_RMSD': mean_rmsd\n",
    "})\n",
    "\n",
    "# Handle missing values (if any)\n",
    "X = X.dropna()\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert X_scaled back to DataFrame\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Define feature weights (replace these with your actual weights)\n",
    "feature_weights = {\n",
    "    'Mean_Delta_Theta': 1,    # Adjust weights based on your PCA results\n",
    "    'Mean_Delta_Phi': 1,\n",
    "    'Mean_Helix_Polarity': 1,\n",
    "    'Mean_RMSD': 1\n",
    "}\n",
    "\n",
    "# Apply feature weights\n",
    "for feature in X_scaled_df.columns:\n",
    "    weight = feature_weights.get(feature)\n",
    "    X_scaled_df[feature] *= weight\n",
    "\n",
    "# Convert back to NumPy array for clustering algorithms\n",
    "X_weighted = X_scaled_df.values\n",
    "\n",
    "# Perform HDBSCAN clustering\n",
    "hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
    "labels = hdbscan_clusterer.fit_predict(X_weighted)\n",
    "\n",
    "# Add cluster labels to data\n",
    "data['Cluster'] = labels\n",
    "\n",
    "# For plotting, extract relevant columns\n",
    "plot_data = pd.DataFrame({\n",
    "    'Mean_Delta_Theta': mean_delta_theta,\n",
    "    'Mean_Delta_Phi': mean_delta_phi,\n",
    "    'Cluster': labels,\n",
    "    'Mean_Helix_Polarity': mean_helix_polarity,\n",
    "    'Mean_RMSD': mean_rmsd  # This should be the transformed Mean_RMSD\n",
    "})\n",
    "\n",
    "\n",
    "# Define colors for clusters, including a color for outliers (-1)\n",
    "unique_labels = np.unique(labels)\n",
    "num_clusters = len(unique_labels)\n",
    "colors = sns.color_palette('Set1', n_colors=num_clusters)\n",
    "\n",
    "# Map labels to colors, handling the -1 label as outliers\n",
    "color_dict = {label: colors[i % len(colors)] for i, label in enumerate(unique_labels)}\n",
    "color_dict[-1] = 'gray'  # Use gray for outliers (label -1)\n",
    "\n",
    "# Create a figure with a 2D plot and a 3D plot\n",
    "fig = plt.figure(figsize=(18, 7), constrained_layout=True)\n",
    "\n",
    "# Add a central title\n",
    "fig.suptitle('HDBScan Clustering', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 3D Scatter Plot\n",
    "ax3d = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "\n",
    "# Iterate over unique labels instead of range(num_clusters)\n",
    "for label in unique_labels:\n",
    "    cluster_data = plot_data[plot_data['Cluster'] == label]\n",
    "    ax3d.scatter(cluster_data['Mean_Delta_Theta'], \n",
    "                 cluster_data['Mean_Delta_Phi'], \n",
    "                 cluster_data['Mean_Helix_Polarity'],\n",
    "                 color=color_dict[label], edgecolor='k', s=50,\n",
    "                 label=f'Cluster {label}' if label != -1 else 'Outliers')\n",
    "\n",
    "# Set labels for the axes and the title\n",
    "ax3d.set_xlabel(r'$\\boldsymbol{\\Delta\\Theta} \\, \\boldsymbol{(°)}$', fontsize=15, fontweight='bold')\n",
    "ax3d.set_ylabel(r'$\\boldsymbol{\\Delta\\Phi} \\, \\boldsymbol{(°)}$', fontsize=15, fontweight='bold')\n",
    "ax3d.set_zlabel('Helix Polarity', fontsize=15, fontweight='bold')\n",
    "ax3d.set_title('3D Conformational Space (ΔΘ, ΔΦ, Polarity)', fontsize=16, fontweight='bold', x=0.6)\n",
    "\n",
    "# Adjust the thickness of the gridlines and axis lines\n",
    "ax3d.tick_params(axis='x', colors='black', width=1.5)\n",
    "ax3d.tick_params(axis='y', colors='black', width=1.5)\n",
    "ax3d.tick_params(axis='z', colors='black', width=1.5)\n",
    "\n",
    "ax3d.grid(True, linewidth=1.5)\n",
    "ax3d.grid(color='gray', linestyle='--', linewidth=0.3)\n",
    "\n",
    "# Make axis lines black and thicker\n",
    "ax3d.xaxis._axinfo[\"axisline\"][\"color\"] = (0,0,0,1)  # Black\n",
    "ax3d.xaxis._axinfo[\"axisline\"][\"linewidth\"] = 2.0\n",
    "\n",
    "ax3d.yaxis._axinfo[\"axisline\"][\"color\"] = (0,0,0,1)  # Black\n",
    "ax3d.yaxis._axinfo[\"axisline\"][\"linewidth\"] = 2.0\n",
    "\n",
    "ax3d.zaxis._axinfo[\"axisline\"][\"color\"] = (0,0,0,1)  # Black\n",
    "ax3d.zaxis._axinfo[\"axisline\"][\"linewidth\"] = 2.0\n",
    "\n",
    "# Adjust the ticks for the axes\n",
    "ax3d.set_zticks([-1, 0, 1])  # Helix polarity axis: only +1 and -1\n",
    "ax3d.set_yticks(np.arange(-200, 201, 100))  # Delta Phi axis: ticks in -100 steps\n",
    "\n",
    "# Adjust tick size and font\n",
    "ax3d.tick_params(axis='both', which='major', length=8, width=1.5, labelsize=12)\n",
    "ax3d.tick_params(axis='both', which='minor', length=4, width=1.0, labelsize=10)\n",
    "\n",
    "# Reference lines\n",
    "ax3d.plot([0, 0], [-200, 200], [1.0, 1.0], color='black', linestyle=(0, (5, 5)), linewidth=1.2)\n",
    "ax3d.plot([-80, 40], [0, 0], [1.0, 1.0], color='black', linestyle=(0, (5, 5)), linewidth=1.2)\n",
    "ax3d.plot([0, 0], [0, 0], [-1, 1.2], color='black', linestyle=(0, (5, 5)), linewidth=1.2)\n",
    "\n",
    "#Add legend\n",
    "ax3d.legend(loc='lower right', fontsize=10, bbox_to_anchor=(1.15, 0))\n",
    "\n",
    "# 2D Scatter Plot\n",
    "ax2d = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "for label in unique_labels:\n",
    "    cluster_data = data[data['Cluster'] == label]\n",
    "    ax2d.scatter(cluster_data['Mean_Delta_Theta'], cluster_data['Mean_Delta_Phi'],\n",
    "                 color=color_dict[label], edgecolor='k', s=50,\n",
    "                 label=f'Cluster {label}' if label != -1 else 'Outliers')\n",
    "\n",
    "ax2d.set_xlabel(r'$\\boldsymbol{\\Delta\\Theta} \\, \\boldsymbol{(°)}$', fontsize=13, fontweight='bold')\n",
    "ax2d.set_ylabel(r'$\\boldsymbol{\\Delta\\Phi} \\, \\boldsymbol{(°)}$', fontsize=13, fontweight='bold')\n",
    "ax2d.set_title('2D Scatter Plot', fontsize=14, fontweight='bold')\n",
    "ax2d.grid(color='gray', linestyle='--', linewidth=0.3)\n",
    "\n",
    "ax2d.axhline(0, color='black', linestyle=(0, (5, 5)), linewidth=1.2)  # Dashed Phi=0 line\n",
    "ax2d.axvline(0, color='black', linestyle=(0, (5, 5)), linewidth=1.2)  # Dashed Theta=0 line\n",
    "\n",
    "# Adjust tick size and font\n",
    "ax2d.tick_params(axis='both', which='major', length=8, width=1.5, labelsize=12)  # Major ticks\n",
    "ax2d.tick_params(axis='both', which='minor', length=4, width=1.0, labelsize=10)  # Minor ticks (optional)\n",
    "\n",
    "# Adjust graph (line) thickness\n",
    "ax2d.spines['top'].set_linewidth(1.5)  # Top border\n",
    "ax2d.spines['right'].set_linewidth(1.5)  # Right border\n",
    "ax2d.spines['left'].set_linewidth(1.5)  # Left border\n",
    "ax2d.spines['bottom'].set_linewidth(1.5)  # Bottom border\n",
    "\n",
    "ax2d.legend()\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "output_file_name = 'all_combined_results_with_HDBScan_clusters.csv'  # Desired output file name\n",
    "output_filtered_csv_path = os.path.join(output_directory, output_file_name)  # Combine directory and file name\n",
    "data.to_csv(output_filtered_csv_path, index=False)\n",
    "\n",
    "# Save the figure (optional)\n",
    "fig.savefig(os.path.join(output_directory, \"HDBScan_Cluster.eps\"), format='eps', dpi=300, bbox_inches=\"tight\")\n",
    "fig.savefig(os.path.join(output_directory, \"HDBScan_Cluster.tif\"), format='tif', dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "# Read the clustered data\n",
    "data = pd.read_csv(output_filtered_csv_path)  # Replace with your actual file path\n",
    "\n",
    "# Relevant columns\n",
    "metrics = [\n",
    "    \"PSSM_Score\",\n",
    "    \"Mean_ipTM\",\n",
    "    \"Mean_RMSD\",\n",
    "    \"Helix_Score\",\n",
    " #   \"Coil_Score\",\n",
    " #   \"Strand_Score\",\n",
    "    \"IUPRED_Score\",\n",
    "    \"ANCHOR_Score\",\n",
    "#  \"Mean_Delta_Theta\",\n",
    "#    \"Mean_Delta_Phi\",\n",
    "#    \"Mean_Helix_Polarity\",\n",
    "]\n",
    "\n",
    "# Ensure 'Cluster' column exists\n",
    "if \"Cluster\" not in data.columns:\n",
    "    raise ValueError(\"The 'Cluster' column is not in the dataset. Please check the input file.\")\n",
    "\n",
    "# Group by cluster and calculate mean and SEM\n",
    "cluster_stats = data.groupby(\"Cluster\")[metrics].agg([\"mean\", \"sem\"]).reset_index()\n",
    "\n",
    "# Flatten MultiIndex columns for easier referencing\n",
    "cluster_stats.columns = [f\"{col[0]}_{col[1]}\" if col[1] else col[0] for col in cluster_stats.columns]\n",
    "\n",
    "# Custom y-axis limits for certain metrics\n",
    "custom_y_limits = {\n",
    "    \"Mean_ipTM\": (0, 1),           # Example: [Min, Max]\n",
    "    \"Helix_Score\": (0, 1),\n",
    "    \"Coil_Score\": (0, 1),\n",
    "    \"Strand_Score\": (0, 1),\n",
    "    \"IUPRED_Score\": (0, 1),\n",
    "    \"ANCHOR_Score\": (0, 1),\n",
    "    \"Mean_Delta_Theta\": (-180, 180),\n",
    "    \"Mean_Delta_Phi\": (-180, 180),\n",
    "    \"Mean_Helix_Polarity\": (-1.1, 1.1),\n",
    "}\n",
    "\n",
    "# Custom titles for metrics with Greek letters and descriptive names\n",
    "metric_titles = {\n",
    "    \"PSSM_Score\": r\"PSSM Score\",\n",
    "    \"Mean_ipTM\": r\"ipTM Score\",\n",
    "    \"Mean_RMSD\": r\"RMSD (Å)\",  # Example: Adding Å for RMSD in angstroms\n",
    "    \"Helix_Score\": r\"Helix Score\",\n",
    "    \"Coil_Score\": r\"Coil Score\",\n",
    "    \"Strand_Score\": r\"Strand Score\",\n",
    "    \"IUPRED_Score\": r\"IUPRED Score\",\n",
    "    \"ANCHOR_Score\": r\"ANCHOR Score\",\n",
    "    \"Mean_Delta_Theta\": r\"$\\boldsymbol{\\Delta\\Theta} \\, \\boldsymbol{(°)}$\",  # Greek delta for Theta\n",
    "    \"Mean_Delta_Phi\": r\"$\\boldsymbol{\\Delta\\Phi} \\, \\boldsymbol{(°)}$\",      # Greek delta for Phi\n",
    "    \"Mean_Helix_Polarity\": r\"Helix Polarity\",\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "sns.set(style=\"whitegrid\")  # Use a clean white grid for the background\n",
    "fig, axes = plt.subplots(1, len(metrics), figsize=(25, 5))\n",
    "\n",
    "# Adjust subplot spacing\n",
    "fig.subplots_adjust(left=0.05, right=0.95, top=0.9, bottom=0.15, wspace=0.4)  # Increase wspace for horizontal spacing\n",
    "\n",
    "# Define a pastel color palette\n",
    "palette = sns.color_palette(\"pastel\", n_colors=len(metrics))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    means = cluster_stats[f\"{metric}_mean\"]\n",
    "    sems = cluster_stats[f\"{metric}_sem\"]\n",
    "    clusters = cluster_stats[\"Cluster\"]\n",
    "    \n",
    "    # Bar plot with error bars\n",
    "    bars = ax.bar(clusters, means, yerr=sems, capsize=3, color=palette[i], edgecolor=\"black\")\n",
    "    \n",
    "    # Set title and labels\n",
    "    title = metric_titles.get(metric, metric).replace(\"_\", \" \")\n",
    "    ax.set_title(title, fontsize=21, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Cluster\", fontsize=20, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Mean Value\", fontsize=20, fontweight=\"bold\")\n",
    "    \n",
    "    # Apply custom y-axis limits if specified\n",
    "    if metric in custom_y_limits:\n",
    "        ymin, ymax = custom_y_limits[metric]\n",
    "        ax.set_ylim(ymin, ymax)\n",
    "    \n",
    "    # Explicitly set x-axis ticks and labels\n",
    "    ax.set_xticks(clusters)  # Set ticks at the cluster positions\n",
    "    ax.set_xticklabels(clusters.astype(int), fontsize=18)  # Ensure labels match the cluster numbers\n",
    "    \n",
    "    # Explicitly set y-axis ticks\n",
    "    yticks = np.linspace(ax.get_ylim()[0], ax.get_ylim()[1], 5)  # Generate 5 evenly spaced ticks\n",
    "    ax.set_yticks(yticks)  # Set y-ticks\n",
    "    ax.set_yticklabels([f\"{tick:.2f}\" for tick in yticks], fontsize=18)  # Format y-tick labels\n",
    "    \n",
    "    # Enable grid only for the y-axis (horizontal lines)\n",
    "    ax.grid(axis=\"y\", color=\"gray\", linestyle=\"--\", linewidth=0.5)  # Horizontal lines only\n",
    "    ax.grid(axis=\"x\", visible=False)  # Disable x-axis grid lines\n",
    "\n",
    "\n",
    "    # Adjust the line thickness and colors\n",
    "    ax.spines['bottom'].set_linewidth(1.5)  \n",
    "    ax.spines['left'].set_linewidth(1.5)    \n",
    "    ax.spines['right'].set_linewidth(1.5)   \n",
    "    ax.spines['top'].set_linewidth(1.5)     \n",
    "\n",
    "    ax.spines['bottom'].set_color('black')  \n",
    "    ax.spines['left'].set_color('black')    \n",
    "    ax.spines['right'].set_color('black')   \n",
    "    ax.spines['top'].set_color('black')     \n",
    "\n",
    "# Add a central title\n",
    "fig.suptitle(\"Cluster-wise Metrics of K-Means\", fontsize=18, fontweight=\"bold\", y=1.02)\n",
    "\n",
    "# Save the figure (optional)\n",
    "fig.savefig(os.path.join(output_directory, \"HDBScan_Cluster_Metadata.eps\"), format='eps', dpi=300, bbox_inches=\"tight\")\n",
    "fig.savefig(os.path.join(output_directory, \"HDBScan_Cluster_Metadata.tif\"), format='tif', dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Extract and Visualize PDB Files for specific Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Map user choices to corresponding CSV files\n",
    "csv_options = {\n",
    "    'h': 'all_combined_results_with_HDBScan_clusters.csv',\n",
    "    'k': 'all_combined_results_with_Kmeans_clusters.csv',\n",
    "    'a': 'all_combined_results_with_Agglomerative_clusters.csv'\n",
    "}\n",
    "\n",
    "# Ask the user which CSV to use\n",
    "clustering_choice = input(\"Enter 'h' for HDBScan, 'k' for K-means, or 'a' for Agglomerative: \").strip().lower()\n",
    "\n",
    "if clustering_choice not in csv_options:\n",
    "    raise ValueError(\"Invalid choice. Please enter 'h', 'k', or 'a'.\")\n",
    "\n",
    "# Build the full CSV file path\n",
    "csv_file_path = os.path.join(output_directory, csv_options[clustering_choice])\n",
    "\n",
    "# Set the cluster you want to open\n",
    "open_cluster = int(input(\"Enter the cluster number for loading structures in PyMOL: \").strip())\n",
    "\n",
    "# Read the selected CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Filter the DataFrame to only include entries from the desired cluster\n",
    "cluster_df = df[df['Cluster'] == open_cluster]\n",
    "\n",
    "# Initialize a list to store PDB file paths\n",
    "pdb_files_to_load = []\n",
    "\n",
    "# Iterate over the entries in the cluster\n",
    "for index, row in cluster_df.iterrows():\n",
    "    folder_name = row['Folder']  # Assuming 'Folder' column has folder names\n",
    "    # Construct the full path to the folder\n",
    "    folder_path = os.path.join(zip_files_folder, folder_name)\n",
    "    # Check if the folder exists\n",
    "    if os.path.exists(folder_path):\n",
    "        # Search for PDB files containing 'unrelaxed_rank_001' in their names\n",
    "        pdb_pattern = os.path.join(folder_path, '**', '*unrelaxed_rank_001*.pdb')\n",
    "        pdb_files = glob.glob(pdb_pattern, recursive=True)\n",
    "        # Normalize paths to ensure consistency\n",
    "        pdb_files = [os.path.normpath(pdb) for pdb in pdb_files]\n",
    "        # Add PDB files to the list\n",
    "        pdb_files_to_load.extend(pdb_files)\n",
    "    else:\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "\n",
    "# Create a filename with the cluster name included. \n",
    "pymol_script_path = os.path.join(output_directory, f\"HDBScan_cluster_{open_cluster}_structures.pml\")\n",
    "\n",
    "# Write a PyMOL script to load all the PDB files\n",
    "with open(pymol_script_path, 'w') as f:\n",
    "    for pdb_file in pdb_files_to_load:\n",
    "        # Escape backslashes in the path for PyMOL compatibility\n",
    "        escaped_path = pdb_file.replace('\\\\', '/')\n",
    "        # Write the PyMOL command to load the PDB file\n",
    "        f.write(f'load \"{escaped_path}\"\\n')\n",
    "\n",
    "print(f\"PyMOL script written to {pymol_script_path}\")\n",
    "print(f\"Number of structures to load: {len(pdb_files_to_load)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Visualize and Confirm Structural Alignment in PyMOL \n",
    "command you can use for that: \n",
    "```\n",
    "for obj in cmd.get_object_list(): cmd.align(f\"{obj} and chain B\", \"c_NP_002211.1_pos_31_unrelaxed_rank_001_alphafold2_multimer_v3_model_4_seed_000 and chain B\")\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```\n",
    "for obj in cmd.get_object_list(): cmd.align(f\"{obj} and chain B\", \"c_NP_002211.1_pos_31 and chain B\")\n",
    "```\n",
    "\n",
    "\n",
    "or save this as a script, as the above commands might not work: \n",
    "```\n",
    "for obj in cmd.get_object_list():\n",
    "    cmd.align(f\"model '{obj}' and chain B\", \"model 'c_NP_002211.1_pos_31' and chain B\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Generate FASTA File for Clustered Hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from Bio.PDB import PDBParser, is_aa\n",
    "from Bio.SeqUtils import seq1\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqIO\n",
    "\n",
    "# Map user choices to corresponding CSV files\n",
    "csv_options = {\n",
    "    'h': 'all_combined_results_with_HDBScan_clusters.csv',\n",
    "    'k': 'all_combined_results_with_Kmeans_clusters.csv',\n",
    "    'a': 'all_combined_results_with_Agglomerative_clusters.csv'\n",
    "}\n",
    "\n",
    "# Prompt user for the method\n",
    "clustering_choice = input(\"Enter 'h' for HDBScan, 'k' for K-means, or 'a' for Agglomerative: \").strip().lower()\n",
    "if clustering_choice not in csv_options:\n",
    "    raise ValueError(\"Invalid choice. Please enter 'h', 'k', or 'a'.\")\n",
    "\n",
    "# Prompt for the cluster number\n",
    "open_cluster = int(input(\"Enter the cluster number for extracting sequences: \").strip())\n",
    "\n",
    "# Determine CSV file path based on user choice\n",
    "csv_file_path = os.path.join(output_directory, csv_options[clustering_choice])\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Filter the DataFrame to only include entries from the desired cluster\n",
    "cluster_df = df[df['Cluster'] == open_cluster]\n",
    "\n",
    "# Initialize a list to store SeqRecord objects\n",
    "seq_records = []\n",
    "\n",
    "# Create a PDBParser\n",
    "parser = PDBParser(QUIET=True)\n",
    "\n",
    "# Iterate over the entries in the cluster\n",
    "for index, row in cluster_df.iterrows():\n",
    "    folder_name = row['Folder']  # Assuming 'Folder' column has folder names\n",
    "    # Extract additional information from the row\n",
    "    name = row.get('Name', 'N/A')\n",
    "    position = row.get('Position', 'N/A')\n",
    "    pssm_score = row.get('PSSM_Score', 'N/A')\n",
    "    iupred_score = row.get('IUPRED_Score', 'N/A')\n",
    "    anchor_score = row.get('ANCHOR_Score', 'N/A')\n",
    "    coil_score = row.get('Coil_Score', 'N/A')\n",
    "    helix_score = row.get('Helix_Score', 'N/A')\n",
    "    strand_score = row.get('Strand_Score', 'N/A')\n",
    "    mean_pLDDT = row.get('Mean_pLDDT', 'N/A')\n",
    "    mean_pTM = row.get('Mean_pTM', 'N/A')\n",
    "    mean_ipTM = row.get('Mean_ipTM', 'N/A')\n",
    "    mean_RMSD = row.get('Mean_RMSD', 'N/A')\n",
    "    mean_Delta_Theta = row.get('Mean_Delta_Theta', 'N/A')\n",
    "    mean_Delta_Phi = row.get('Mean_Delta_Phi', 'N/A')\n",
    "    mean_Helix_Polarity = row.get('Mean_Helix_Polarity', 'N/A')\n",
    "\n",
    "    # Construct a description string with all the information\n",
    "    description = (\n",
    "        f\"Name:{name} Position:{position} PSSM_Score:{pssm_score} \"\n",
    "        f\"IUPRED_Score:{iupred_score} ANCHOR_Score:{anchor_score} \"\n",
    "        f\"Coil_Score:{coil_score} Helix_Score:{helix_score} Strand_Score:{strand_score} \"\n",
    "        f\"Mean_pLDDT:{mean_pLDDT} Mean_pTM:{mean_pTM} Mean_ipTM:{mean_ipTM} \"\n",
    "        f\"RMSD:{mean_RMSD} Delta_Theta:{mean_Delta_Theta} \"\n",
    "        f\"Delta_Phi:{mean_Delta_Phi} Helix_Polarity:{mean_Helix_Polarity}\"\n",
    "    )\n",
    "\n",
    "    # Construct the full path to the folder\n",
    "    folder_path = os.path.join(zip_files_folder, folder_name)\n",
    "    # Check if the folder exists\n",
    "    if os.path.exists(folder_path):\n",
    "        # Search for PDB files containing 'unrelaxed_rank_001' in their names\n",
    "        pdb_pattern = os.path.join(folder_path, '**', '*unrelaxed_rank_001*.pdb')\n",
    "        pdb_files = glob.glob(pdb_pattern, recursive=True)\n",
    "        # Normalize paths to ensure consistency\n",
    "        pdb_files = [os.path.normpath(pdb) for pdb in pdb_files]\n",
    "        # Process each PDB file\n",
    "        for pdb_file in pdb_files:\n",
    "            try:\n",
    "                structure = parser.get_structure('', pdb_file)\n",
    "                # Assume model 0\n",
    "                model = structure[0]\n",
    "                # Get chain A\n",
    "                chainA = model['A']\n",
    "                # Extract the sequence\n",
    "                residues = []\n",
    "                for residue in chainA:\n",
    "                    if is_aa(residue, standard=True):\n",
    "                        res_name = residue.get_resname()\n",
    "                        # Convert three-letter code to one-letter code\n",
    "                        res_short = seq1(res_name)\n",
    "                        residues.append(res_short)\n",
    "                seq_one_letter = ''.join(residues)\n",
    "                # Create a SeqRecord with an informative ID and full description\n",
    "                seq_id = os.path.basename(pdb_file).split('.')[0]  # Filename without extension\n",
    "                seq_record = SeqRecord(Seq(seq_one_letter), id=seq_id, description=description)\n",
    "                seq_records.append(seq_record)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing PDB file {pdb_file}: {e}\")\n",
    "    else:\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "\n",
    "# Write sequences to a FASTA file with separate entries\n",
    "fasta_output_path = os.path.join(output_directory, f'cluster_{open_cluster}_sequences.fasta')\n",
    "\n",
    "with open(fasta_output_path, 'w') as f:\n",
    "    SeqIO.write(seq_records, f, 'fasta')\n",
    "\n",
    "print(f\"\\nSequences from cluster {open_cluster} have been written to:\\n{fasta_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Retrieve Gene names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_fasta =   # Redundant fasta file\n",
    "non_redundant_fasta = fasta_output_path  # Non-redundant sequences\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio import Entrez\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# Configure NCBI email and API key\n",
    "Entrez.email = # Replace with your email\n",
    "Entrez.api_key = # Replace with your API key (optional)\n",
    "\n",
    "\n",
    "# Dynamically construct output file paths\n",
    "output_fasta = os.path.join(output_directory, \"annotated_sequences.fasta\")  # Annotated sequences\n",
    "output_summary = os.path.join(output_directory, \"sequence_gene_names.txt\")  # Sequence to gene names mapping\n",
    "output_gene_list = os.path.join(output_directory, \"unique_gene_names.txt\")  # List of unique gene names\n",
    "\n",
    "def extract_protein_id(record):\n",
    "    \"\"\"\n",
    "    Extract the protein ID from the record.\n",
    "    \"\"\"\n",
    "    # Try to get protein ID from 'Name:' in description\n",
    "    for part in record.description.split():\n",
    "        if \"Name:\" in part:\n",
    "            return part.split(\"Name:\")[1]\n",
    "    # If not found, use record.id\n",
    "    return record.id\n",
    "\n",
    "def fetch_gene_name(protein_id, gene_name_cache):\n",
    "    \"\"\"\n",
    "    Fetch the gene name from NCBI using the protein ID, with caching.\n",
    "    \"\"\"\n",
    "    if protein_id in gene_name_cache:\n",
    "        return gene_name_cache[protein_id]\n",
    "    try:\n",
    "        # Use Entrez.efetch to get the protein record in XML format\n",
    "        handle = Entrez.efetch(db=\"protein\", id=protein_id, rettype=\"gp\", retmode=\"xml\")\n",
    "        records = Entrez.read(handle)\n",
    "        handle.close()\n",
    "        \n",
    "        # Navigate the XML structure to find the gene name\n",
    "        gene_name = \"Unknown\"\n",
    "        for feature in records[0]['GBSeq_feature-table']:\n",
    "            if feature['GBFeature_key'] == 'Gene':\n",
    "                for qualifier in feature['GBFeature_quals']:\n",
    "                    if qualifier['GBQualifier_name'] == 'gene':\n",
    "                        gene_name = qualifier['GBQualifier_value']\n",
    "                        gene_name_cache[protein_id] = gene_name\n",
    "                        return gene_name\n",
    "            elif feature['GBFeature_key'] == 'CDS':\n",
    "                for qualifier in feature['GBFeature_quals']:\n",
    "                    if qualifier['GBQualifier_name'] == 'gene':\n",
    "                        gene_name = qualifier['GBQualifier_value']\n",
    "                        gene_name_cache[protein_id] = gene_name\n",
    "                        return gene_name\n",
    "        gene_name_cache[protein_id] = gene_name\n",
    "        return gene_name\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching gene name for {protein_id}: {e}\")\n",
    "        gene_name_cache[protein_id] = \"Error\"\n",
    "        return \"Error\"\n",
    "    finally:\n",
    "        time.sleep(0.1)  # Adjust based on NCBI rate limits\n",
    "\n",
    "# Step 1: Build a mapping from sequence string to set of protein IDs from the redundant fasta file\n",
    "sequence_to_protein_ids = defaultdict(set)\n",
    "\n",
    "with open(redundant_fasta, \"r\") as red_file:\n",
    "    for record in SeqIO.parse(red_file, \"fasta\"):\n",
    "        seq_str = str(record.seq)\n",
    "        protein_id = extract_protein_id(record)\n",
    "        sequence_to_protein_ids[seq_str].add(protein_id)\n",
    "\n",
    "# Initialize data structures\n",
    "annotated_records = []\n",
    "sequence_to_gene_names = {}\n",
    "processed_count = 0\n",
    "gene_name_cache = {}  # Cache to avoid redundant API calls\n",
    "\n",
    "# Step 2: Process the non-redundant fasta file\n",
    "with open(non_redundant_fasta, \"r\") as nr_file:\n",
    "    for record in SeqIO.parse(nr_file, \"fasta\"):\n",
    "        seq_str = str(record.seq)\n",
    "        matching_protein_ids = sequence_to_protein_ids.get(seq_str, set())\n",
    "        gene_names = set()\n",
    "        for protein_id in matching_protein_ids:\n",
    "            gene_name = fetch_gene_name(protein_id, gene_name_cache)\n",
    "            if gene_name not in [\"Unknown\", \"Error\"]:\n",
    "                gene_names.add(gene_name)\n",
    "        # Annotate the record with the gene names\n",
    "        gene_names_str = \", \".join(sorted(gene_names)) if gene_names else \"Unknown\"\n",
    "        record.description += f\" | Gene(s): {gene_names_str}\"\n",
    "        # Add the record to the list\n",
    "        annotated_records.append(record)\n",
    "        # Store the mapping for the summary\n",
    "        sequence_to_gene_names[record.id] = {\n",
    "            'protein_ids': matching_protein_ids,\n",
    "            'gene_names': gene_names if gene_names else {\"Unknown\"}\n",
    "        }\n",
    "        processed_count += 1\n",
    "        # Print progress every 10 sequences\n",
    "        if processed_count % 10 == 0:\n",
    "            print(f\"Processed {processed_count} sequences\")\n",
    "\n",
    "# Step 3: Write the annotated sequences to the output FASTA file\n",
    "with open(output_fasta, \"w\") as output_fasta_file:\n",
    "    SeqIO.write(annotated_records, output_fasta_file, \"fasta\")\n",
    "\n",
    "# Step 4: Write the sequence to gene names mapping to a summary file\n",
    "with open(output_summary, \"w\") as summary_file:\n",
    "    for seq_id, info in sequence_to_gene_names.items():\n",
    "        protein_ids_str = \", \".join(info['protein_ids'])\n",
    "        gene_names_str = \", \".join(sorted(info['gene_names'])) if info['gene_names'] else \"Unknown\"\n",
    "        summary_file.write(f\"Sequence ID: {seq_id}\\n\")\n",
    "        summary_file.write(f\"Protein IDs: {protein_ids_str}\\n\")\n",
    "        summary_file.write(f\"Gene Names: {gene_names_str}\\n\")\n",
    "        summary_file.write(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "# Step 5: Create a set of unique gene names\n",
    "unique_genes = set()\n",
    "for info in sequence_to_gene_names.values():\n",
    "    unique_genes.update(info['gene_names'])\n",
    "\n",
    "# Write the unique gene names to a file\n",
    "with open(output_gene_list, \"w\") as gene_list_file:\n",
    "    for gene_name in sorted(unique_genes):\n",
    "        gene_list_file.write(f\"{gene_name}\\n\")\n",
    "\n",
    "# Print summary statistics\n",
    "total_sequences = len(sequence_to_gene_names)\n",
    "print(f\"Summary:\")\n",
    "print(f\" - Total sequences processed: {total_sequences}\")\n",
    "print(f\" - Total unique genes found: {len(unique_genes)}\")\n",
    "print(f\"Results written to:\")\n",
    "print(f\" - Annotated FASTA: {output_fasta}\")\n",
    "print(f\" - Sequence to Gene Names Mapping: {output_summary}\")\n",
    "print(f\" - Unique Gene Names List: {output_gene_list}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SLiMFold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
